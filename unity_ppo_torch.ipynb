{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "from torch import multiprocessing\n",
    "from torch import nn\n",
    "from tensordict.nn import TensorDictModule\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from torch import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_fork = multiprocessing.get_start_method() == \"fork\"\n",
    "\n",
    "device = (\n",
    "    torch.device(0)\n",
    "    if torch.cuda.is_available() and not is_fork\n",
    "    else torch.device(\"cpu\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load unity environment using `mlagents_envs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlagents_envs.side_channel.engine_configuration_channel import EngineConfigurationChannel\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "\n",
    "channel = EngineConfigurationChannel()\n",
    "env_path = \"C:/Users/Pawel/Documents/Unity_Project/warehouse-bot-training/environment_builds/test_env_simplified/Warehouse_Bot.exe\"\n",
    "\n",
    "from torchrl.envs import UnityMLAgentsEnv\n",
    "\n",
    "unity_env = UnityEnvironment(\n",
    "  file_name=env_path,\n",
    "  side_channels=[channel],\n",
    "  # additional_args=[\"-batchmode\", \"-nographics\"]\n",
    ")\n",
    "channel.set_configuration_parameters(time_scale=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform environment from `mlagents` to `gymnasium`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.0\n"
     ]
    }
   ],
   "source": [
    "print(gym.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from mlagents_envs.base_env import ActionTuple\n",
    "\n",
    "class UnityGymWrapper(gym.Env):\n",
    "    def __init__(self, unity_env, seed=None):\n",
    "        super().__init__()\n",
    "        self.unity_env = unity_env\n",
    "        self.unity_env.reset()\n",
    "        self.behavior_name = list(self.unity_env.behavior_specs.keys())[0]\n",
    "        self.spec = self.unity_env.behavior_specs[self.behavior_name]   \n",
    "        \n",
    "        # Define observation space (assuming visual input)\n",
    "        obs_shape = self.spec.observation_specs[0].shape\n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=obs_shape, dtype=np.uint8) # ???\n",
    "        \n",
    "        # Define action space\n",
    "        # if self.spec.action_spec.is_continuous():\n",
    "            # self.action_space = spaces.Box(\n",
    "            #     low=self.spec.action_spec.continuous_action_spec[0],\n",
    "            #     high=self.spec.action_spec.continuous_action_spec[1],\n",
    "            #     shape=(self.spec.action_spec.continuous_size,),\n",
    "            #     dtype=np.float32\n",
    "            # )\n",
    "        if self.spec.action_spec.is_discrete():\n",
    "            self.action_space = spaces.Discrete(self.spec.action_spec.discrete_branches[0])\n",
    "\n",
    "        \n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.unity_env.reset()\n",
    "        decision_steps, _ = self.unity_env.get_steps(self.behavior_name)\n",
    "        obs = decision_steps.obs[0]  # Assuming single-agent scenario\n",
    "        return obs, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        action_tuple = ActionTuple()\n",
    "        # if self.spec.action_spec.is_continuous():\n",
    "        #     action_tuple.add_continuous(np.array(action).reshape(1, -1))\n",
    "        # else:\n",
    "        #     action_tuple.add_discrete(np.array(action).reshape(1, -1))\n",
    "        \n",
    "        if self.spec.action_spec.is_discrete():\n",
    "            action_tuple.add_discrete(np.array(action).reshape(1, -1))\n",
    "        \n",
    "        # print(action_tuple, np.array(action).reshape(1, -1))\n",
    "        self.unity_env.set_action_for_agent(self.behavior_name, 0, action_tuple)\n",
    "        self.unity_env.step()\n",
    "        \n",
    "        decision_steps, terminal_steps = self.unity_env.get_steps(self.behavior_name)\n",
    "\n",
    "        if 0 in terminal_steps:\n",
    "            obs = terminal_steps.obs[0]\n",
    "            reward = terminal_steps.reward[0]\n",
    "            \n",
    "            # terminated - Natural episode ending.\n",
    "            terminated = not terminal_steps.interrupted[0]\n",
    "            \n",
    "            # truncated - \"Whether the truncation condition outside the scope of the MDP is satisfied. Typically, this is a timelimit\"\n",
    "            # interrupted - \"The episode ended due to max steps or external termination, not because the episode ended naturally (failed/succeeded).\"\n",
    "            truncated = terminal_steps.interrupted[0]\n",
    "            \n",
    "            # terminated and truncated are mutually exclusive\n",
    "        else:\n",
    "            obs = decision_steps.obs[0]\n",
    "            reward = decision_steps.reward[0]\n",
    "            terminated = False\n",
    "            truncated = False\n",
    "        \n",
    "        return obs, reward, terminated, truncated, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass  # Unity renders its own environment\n",
    "    \n",
    "    def close(self):\n",
    "        self.unity_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "gymnasium_env = UnityGymWrapper(unity_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "LAMBDA = 0.95\n",
    "CLIP_EPS = 0.2\n",
    "LR = 3e-4\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "# ROLLOUT_SIZE = 2048\n",
    "\n",
    "BUFFER_SIZE = 10240\n",
    "TOTAL_STEPS = 102400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        action_probs = self.actor(state)\n",
    "        value = self.critic(state)\n",
    "        return action_probs, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_advantages(rewards, values, dones):\n",
    "    advantages = []\n",
    "    gae = 0\n",
    "    next_value = values[-1]\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        delta = rewards[t] + GAMMA * next_value * (1 - dones[t]) - values[t]\n",
    "        gae = delta + GAMMA * LAMBDA * (1 - dones[t]) * gae\n",
    "        advantages.insert(0, gae)\n",
    "        next_value = values[t]\n",
    "    returns = np.array(advantages) + values[:-1]\n",
    "    return torch.tensor(advantages, dtype=torch.float32, device=device), \\\n",
    "           torch.tensor(returns, dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_update(model, optimizer, states, actions, old_log_probs, returns, advantages):\n",
    "    for _ in range(EPOCHS):\n",
    "        indices = np.random.permutation(len(states))\n",
    "        print(indices)\n",
    "        for i in range(0, len(states), BATCH_SIZE):\n",
    "            batch_idx = indices[i:i + BATCH_SIZE]\n",
    "            print(batch_idx)\n",
    "            \n",
    "            state_batch = states[batch_idx]\n",
    "            action_batch = actions[batch_idx]\n",
    "            old_log_prob_batch = old_log_probs[batch_idx]\n",
    "            return_batch = returns[batch_idx]\n",
    "            advantage_batch = advantages[batch_idx]\n",
    "\n",
    "            action_probs, values = model(state_batch)\n",
    "            dist = Categorical(action_probs)\n",
    "            new_log_probs = dist.log_prob(action_batch)\n",
    "            entropy = dist.entropy().mean()\n",
    "            \n",
    "            ratio = torch.exp(new_log_probs - old_log_prob_batch) # ???\n",
    "            clipped_ratio = torch.clamp(ratio, 1 - CLIP_EPS, 1 + CLIP_EPS)\n",
    "            policy_loss = -torch.min(ratio * advantage_batch, clipped_ratio * advantage_batch).mean()\n",
    "            value_loss = nn.MSELoss()(values.squeeze(), return_batch)\n",
    "            loss = policy_loss + 0.5 * value_loss - 0.01 * entropy\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bufffer(env, model, optimizer):\n",
    "    state, _ = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "    \n",
    "    for i in range(BUFFER_SIZE):\n",
    "        states, actions, rewards, dones, log_probs, values = [], [], [], [], [], []\n",
    "        \n",
    "        # Choose Action\n",
    "        action_probs, value = model(state)\n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        \n",
    "        # Gather trajectories\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        # Fill buffer\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value.item())\n",
    "        \n",
    "        if done:\n",
    "            state, _ = env.reset()\n",
    "        else:\n",
    "            state = torch.tensor(next_state, dtype=torch.float32, device=device)\n",
    "    \n",
    "        # No sense - rewrite\n",
    "        # states = torch.stack(states)\n",
    "        # actions = torch.tensor(actions, dtype=torch.int64, device=device)\n",
    "        # log_probs = torch.stack(log_probs)\n",
    "        # values.append(0)  # Bootstrap last value\n",
    "        \n",
    "        # Compute advantage\n",
    "        advantages, returns = compute_advantages(rewards, values, dones)\n",
    "        print(f\"advantages: {advantages}, returns:{returns}\")\n",
    "        \n",
    "        # Update policy\n",
    "        ppo_update(model, optimizer, states, actions, log_probs, returns, advantages)\n",
    "        \n",
    "        print(f\"Mean Reward: {sum(rewards) / len(rewards)}\")\n",
    "\n",
    "    env.close()\n",
    "    torch.save(model.state_dict(), \"ppo_cartpole.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "advantages: tensor([-0.2029]), returns:tensor([])\n",
      "[0]\n",
      "[0]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m ActorCritic(state_dim, action_dim)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mLR)\n\u001b[1;32m----> 7\u001b[0m \u001b[43mtrain_bufffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgymnasium_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[46], line 42\u001b[0m, in \u001b[0;36mtrain_bufffer\u001b[1;34m(env, model, optimizer)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madvantages: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madvantages\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, returns:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# Update policy\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m     \u001b[43mppo_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madvantages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean Reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(rewards)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(rewards)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     46\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[1;32mIn[45], line 9\u001b[0m, in \u001b[0;36mppo_update\u001b[1;34m(model, optimizer, states, actions, old_log_probs, returns, advantages)\u001b[0m\n\u001b[0;32m      6\u001b[0m batch_idx \u001b[38;5;241m=\u001b[39m indices[i:i \u001b[38;5;241m+\u001b[39m BATCH_SIZE]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch_idx)\n\u001b[1;32m----> 9\u001b[0m state_batch \u001b[38;5;241m=\u001b[39m \u001b[43mstates\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     10\u001b[0m action_batch \u001b[38;5;241m=\u001b[39m actions[batch_idx]\n\u001b[0;32m     11\u001b[0m old_log_prob_batch \u001b[38;5;241m=\u001b[39m old_log_probs[batch_idx]\n",
      "\u001b[1;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "state_dim = gymnasium_env.observation_space.shape[0]\n",
    "action_dim = gymnasium_env.action_space.n\n",
    "model = ActorCritic(state_dim, action_dim).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "train_bufffer(gymnasium_env, model, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
