{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "from torch import multiprocessing\n",
    "from torch import nn\n",
    "from tensordict.nn import TensorDictModule\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from torch import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_fork = multiprocessing.get_start_method() == \"fork\"\n",
    "\n",
    "device = (\n",
    "    torch.device(0)\n",
    "    if torch.cuda.is_available() and not is_fork\n",
    "    else torch.device(\"cpu\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load unity environment using `mlagents_envs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlagents_envs.side_channel.engine_configuration_channel import EngineConfigurationChannel\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "\n",
    "channel = EngineConfigurationChannel()\n",
    "env_path = \"C:/Users/Pawel/Documents/Unity_Project/warehouse-bot-training/environment_builds/test_env_simplified/Warehouse_Bot.exe\"\n",
    "\n",
    "from torchrl.envs import UnityMLAgentsEnv\n",
    "\n",
    "unity_env = UnityEnvironment(\n",
    "  file_name=env_path,\n",
    "  side_channels=[channel],\n",
    "  # additional_args=[\"-batchmode\", \"-nographics\"]\n",
    ")\n",
    "channel.set_configuration_parameters(time_scale=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform environment from `mlagents` to `gymnasium`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gym.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from mlagents_envs.base_env import ActionTuple\n",
    "\n",
    "class UnityGymWrapper(gym.Env):\n",
    "    def __init__(self, unity_env, seed=None):\n",
    "        super().__init__()\n",
    "        self.unity_env = unity_env\n",
    "        self.unity_env.reset()\n",
    "        self.behavior_name = list(self.unity_env.behavior_specs.keys())[0]\n",
    "        self.spec = self.unity_env.behavior_specs[self.behavior_name]   \n",
    "        \n",
    "        # Define observation space (assuming visual input)\n",
    "        obs_shape = self.spec.observation_specs[0].shape\n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=obs_shape, dtype=np.uint8) # ???\n",
    "        \n",
    "        # Define action space\n",
    "        # if self.spec.action_spec.is_continuous():\n",
    "            # self.action_space = spaces.Box(\n",
    "            #     low=self.spec.action_spec.continuous_action_spec[0],\n",
    "            #     high=self.spec.action_spec.continuous_action_spec[1],\n",
    "            #     shape=(self.spec.action_spec.continuous_size,),\n",
    "            #     dtype=np.float32\n",
    "            # )\n",
    "        if self.spec.action_spec.is_discrete():\n",
    "            self.action_space = spaces.Discrete(self.spec.action_spec.discrete_branches[0])\n",
    "\n",
    "        \n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.unity_env.reset()\n",
    "        decision_steps, _ = self.unity_env.get_steps(self.behavior_name)\n",
    "        obs = decision_steps.obs[0]  # Assuming single-agent scenario\n",
    "        return obs, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        action_tuple = ActionTuple()\n",
    "        # if self.spec.action_spec.is_continuous():\n",
    "        #     action_tuple.add_continuous(np.array(action).reshape(1, -1))\n",
    "        # else:\n",
    "        #     action_tuple.add_discrete(np.array(action).reshape(1, -1))\n",
    "        \n",
    "        if self.spec.action_spec.is_discrete():\n",
    "            action_tuple.add_discrete(np.array(action).reshape(1, -1))\n",
    "        \n",
    "        # print(action_tuple, np.array(action).reshape(1, -1))\n",
    "        self.unity_env.set_action_for_agent(self.behavior_name, 0, action_tuple)\n",
    "        self.unity_env.step()\n",
    "        \n",
    "        decision_steps, terminal_steps = self.unity_env.get_steps(self.behavior_name)\n",
    "\n",
    "        if 0 in terminal_steps:\n",
    "            obs = terminal_steps.obs[0]\n",
    "            reward = terminal_steps.reward[0]\n",
    "            \n",
    "            # terminated - Natural episode ending.\n",
    "            terminated = not terminal_steps.interrupted[0]\n",
    "            \n",
    "            # truncated - \"Whether the truncation condition outside the scope of the MDP is satisfied. Typically, this is a timelimit\"\n",
    "            # interrupted - \"The episode ended due to max steps or external termination, not because the episode ended naturally (failed/succeeded).\"\n",
    "            truncated = terminal_steps.interrupted[0]\n",
    "            \n",
    "            # terminated and truncated are mutually exclusive\n",
    "        else:\n",
    "            obs = decision_steps.obs[0]\n",
    "            reward = decision_steps.reward[0]\n",
    "            terminated = False\n",
    "            truncated = False\n",
    "        \n",
    "        return obs, reward, terminated, truncated, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass  # Unity renders its own environment\n",
    "    \n",
    "    def close(self):\n",
    "        self.unity_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gymnasium_env = UnityGymWrapper(unity_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "LAMBDA = 0.95\n",
    "CLIP_EPS = 0.2\n",
    "LR = 3e-4\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "# ROLLOUT_SIZE = 2048\n",
    "\n",
    "BUFFER_SIZE = 10240\n",
    "TOTAL_STEPS = 102400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        action_probs = self.actor(state)\n",
    "        value = self.critic(state)\n",
    "        return action_probs, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_advantages(rewards, values, dones):\n",
    "    advantages = []\n",
    "    gae = 0\n",
    "    next_value = values[-1]\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        delta = rewards[t] + GAMMA * next_value * (1 - dones[t]) - values[t]\n",
    "        gae = delta + GAMMA * LAMBDA * (1 - dones[t]) * gae\n",
    "        advantages.insert(0, gae)\n",
    "        next_value = values[t]\n",
    "    returns = np.array(advantages) + values[:-1]\n",
    "    return torch.tensor(advantages, dtype=torch.float32, device=device), \\\n",
    "           torch.tensor(returns, dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_update(model, optimizer, states, actions, old_log_probs, returns, advantages):\n",
    "    for _ in range(EPOCHS):\n",
    "        indices = np.random.permutation(len(states))\n",
    "        print(indices)\n",
    "        for i in range(0, len(states), BATCH_SIZE):\n",
    "            batch_idx = indices[i:i + BATCH_SIZE]\n",
    "            print(batch_idx)\n",
    "            \n",
    "            state_batch = states[batch_idx]\n",
    "            action_batch = actions[batch_idx]\n",
    "            old_log_prob_batch = old_log_probs[batch_idx]\n",
    "            return_batch = returns[batch_idx]\n",
    "            advantage_batch = advantages[batch_idx]\n",
    "\n",
    "            action_probs, values = model(state_batch)\n",
    "            dist = Categorical(action_probs)\n",
    "            new_log_probs = dist.log_prob(action_batch)\n",
    "            entropy = dist.entropy().mean()\n",
    "            \n",
    "            ratio = torch.exp(new_log_probs - old_log_prob_batch) # ???\n",
    "            clipped_ratio = torch.clamp(ratio, 1 - CLIP_EPS, 1 + CLIP_EPS)\n",
    "            policy_loss = -torch.min(ratio * advantage_batch, clipped_ratio * advantage_batch).mean()\n",
    "            value_loss = nn.MSELoss()(values.squeeze(), return_batch)\n",
    "            loss = policy_loss + 0.5 * value_loss - 0.01 * entropy\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bufffer(env, model, optimizer):\n",
    "    state, _ = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "    \n",
    "    for i in range(BUFFER_SIZE):\n",
    "        states, actions, rewards, dones, log_probs, values = [], [], [], [], [], []\n",
    "        \n",
    "        # Choose Action\n",
    "        action_probs, value = model(state)\n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        \n",
    "        # Gather trajectories\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        # Fill buffer\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value.item())\n",
    "        \n",
    "        if done:\n",
    "            state, _ = env.reset()\n",
    "        else:\n",
    "            state = torch.tensor(next_state, dtype=torch.float32, device=device)\n",
    "    \n",
    "        # No sense - rewrite\n",
    "        # states = torch.stack(states)\n",
    "        # actions = torch.tensor(actions, dtype=torch.int64, device=device)\n",
    "        # log_probs = torch.stack(log_probs)\n",
    "        # values.append(0)  # Bootstrap last value\n",
    "        \n",
    "        # Compute advantage\n",
    "        advantages, returns = compute_advantages(rewards, values, dones)\n",
    "        print(f\"advantages: {advantages}, returns:{returns}\")\n",
    "        \n",
    "        # Update policy\n",
    "        ppo_update(model, optimizer, states, actions, log_probs, returns, advantages)\n",
    "        \n",
    "        print(f\"Mean Reward: {sum(rewards) / len(rewards)}\")\n",
    "\n",
    "    env.close()\n",
    "    torch.save(model.state_dict(), \"ppo_cartpole.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = gymnasium_env.observation_space.shape[0]\n",
    "action_dim = gymnasium_env.action_space.n\n",
    "model = ActorCritic(state_dim, action_dim).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "train_bufffer(gymnasium_env, model, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
