{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DISCLAIMER\n",
    "\n",
    "This is implementation from pyTorch tutorials - https://pytorch.org/tutorials/intermediate/reinforcement_ppo.html\n",
    "\n",
    "They use some weird convention of naming value network as critic network and policy network as actor network. This is not only the matter of this notebook or their tutorial but also the parameters withing torchrl library\n",
    "\n",
    "There is also tutorial using mlagents library - https://github.com/Unity-Technologies/ml-agents/blob/develop/colab/Colab_UnityEnvironment_2_Train.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "from torch import multiprocessing\n",
    "from torch import nn\n",
    "from tensordict.nn import TensorDictModule\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "from torch import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_fork = multiprocessing.get_start_method() == \"fork\"\n",
    "\n",
    "device = (\n",
    "    torch.device(0)\n",
    "    if torch.cuda.is_available() and not is_fork\n",
    "    else torch.device(\"cpu\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cells = 256  # number of cells in each layer i.e. output dim.\n",
    "lr = 3e-4\n",
    "max_grad_norm = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buffer_size = 10000\n",
    "# batch_size = 1000 # Number of frames per batch\n",
    "# For a complete training, bring the number of frames up to 1M\n",
    "total_frames = 100_000 # total size of frames to train on\n",
    "max_steps = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 1024\n",
    "batch_size = 1024\n",
    "sub_batch_size = 512\n",
    "\n",
    "num_epochs = 3  # optimization steps per batch of data collected\n",
    "clip_epsilon = (\n",
    "    0.2  # clip value for PPO loss: see the equation in the intro for more context.\n",
    ")\n",
    "gamma = 0.99\n",
    "lmbda = 0.95\n",
    "\n",
    "# Calculating PPO loss\n",
    "critic_coef = 0.5\n",
    "entropy_eps = 0.005\n",
    "# entropy_eps = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_scale = 2\n",
    "\n",
    "input_size = 52\n",
    "actions_size = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load unity environment using `mlagents_envs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlagents_envs.side_channel.engine_configuration_channel import EngineConfigurationChannel\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "\n",
    "channel = EngineConfigurationChannel()\n",
    "env_path = \"C:/Users/Pawel/Documents/Unity_Project/warehouse-bot-training/environment_builds/test_env_simplified/Warehouse_Bot.exe\"\n",
    "\n",
    "from torchrl.envs import UnityMLAgentsEnv\n",
    "\n",
    "unity_env = UnityEnvironment(\n",
    "  file_name=env_path,\n",
    "  side_channels=[channel],\n",
    "  # additional_args=[\"-batchmode\", \"-nographics\"]\n",
    ")\n",
    "channel.set_configuration_parameters(time_scale=time_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform environment to Gym format from `mlagents_envs` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlagents_envs.envs.unity_gym_env import UnityToGymWrapper\n",
    "# gym_env = UnityToGymWrapper(unity_env, uint8_visual=False, flatten_branched=False, allow_multiple_obs=True)\n",
    "gym_env = UnityToGymWrapper(unity_env, allow_multiple_obs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flattening observations (removing Tuple(Box) -> Box)\n",
    "# from gym.wrappers import FlattenObservation\n",
    "\n",
    "# gym_env = FlattenObservation(gym_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform environment to pure `gym` package format from `mlagents_envs` gym format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from mlagents_envs.base_env import TerminalSteps\n",
    "\n",
    "class UnityGymTorchWrapper(gym.Env):\n",
    "    \"\"\"\n",
    "    Custom wrapper for UnityToGymWrapper to make it compatible with TorchRL's GymWrapper.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, unity_gym_env):\n",
    "        \"\"\"\n",
    "        Takes unity_gym_env - environment from \"mlagents_envs\" package\n",
    "        \"\"\" \n",
    "        super().__init__()\n",
    "        \n",
    "        self.unity_gym_env = unity_gym_env\n",
    "        self.observation_space = unity_gym_env.observation_space\n",
    "        self.action_space = unity_gym_env.action_space\n",
    "        \n",
    "    def reset(self):\n",
    "        observation = self.unity_gym_env.reset()\n",
    "        \n",
    "        if isinstance(observation, list):  \n",
    "            observation = np.concatenate(observation, axis=-1)\n",
    "        \n",
    "        # return observation\n",
    "        return {\"observation\": observation}, {}  # TorchRL expects a dictionary\n",
    "\n",
    "    def step(self, action):\n",
    "        observation, reward, terminated, info = self.unity_gym_env.step(action)\n",
    "        \n",
    "        # Truncated should be true only when episode ended prematurely - didn't reach max step\n",
    "        truncated = False\n",
    "        decision_steps = info['step'] # steps (with s) because by can describe multiple agents\n",
    "        \n",
    "        # Check if episode ended\n",
    "        if isinstance(decision_steps, TerminalSteps):\n",
    "            # True when max steps reached - oposite of truncated\n",
    "            truncated = not decision_steps.interrupted[0]\n",
    "            # print(f\"Did end prematurely? {truncated}\")\n",
    "        \n",
    "        if isinstance(observation, list):  \n",
    "            observation = np.concatenate(observation, axis=-1)\n",
    "        \n",
    "        return {\"observation\": observation}, reward, terminated, truncated, info\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        return self.unity_gym_env.render(mode=mode)\n",
    "\n",
    "    def close(self):\n",
    "        self.unity_gym_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_env = UnityGymTorchWrapper(gym_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform again to gym format but from `torchrl` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.envs import GymEnv\n",
    "torchrl_gym_env = GymEnv(custom_env)\n",
    "# torchrl_gym_env = custom_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = torchrl_gym_env.rollout(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchrl.envs import Compose, DoubleToFloat, ObservationNorm, StepCounter, TransformedEnv\n",
    "\n",
    "env = TransformedEnv(\n",
    "    torchrl_gym_env,\n",
    "    Compose(\n",
    "        ObservationNorm(in_keys=[\"observation\"]), # normalize observations\n",
    "        DoubleToFloat(),\n",
    "        StepCounter(),\n",
    "    ),\n",
    ")\n",
    "\n",
    "env.transform[0].init_stats(num_iter=500, reduce_dim=0, cat_dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"normalization constant shape:\", env.transform[0].loc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"--- observation_spec:\", env.observation_spec)\n",
    "# print(\"--- reward_spec:\", env.reward_spec)\n",
    "# print(\"--- input_spec:\", env.input_spec)\n",
    "# print(\"--- action_spec (as defined by input_spec):\", env.action_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.envs.utils import check_env_specs\n",
    "check_env_specs(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout = env.rollout(100)\n",
    "# print(\"rollout of three steps:\", rollout)\n",
    "print(\"Shape of the rollout TensorDict:\", rollout.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Categorical\n",
    "\n",
    "class Policy_Network(nn.Module):\n",
    "    def __init__(self, num_cells) -> None:\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(input_size, num_cells),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_cells, num_cells),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_cells, env.action_spec.shape[-1])\n",
    "        )\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        logits = self.main(inputs)\n",
    "        output = self.softmax(logits)\n",
    "        # print(logits, output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = Policy_Network(num_cells).to(device)\n",
    "\n",
    "policy_module = TensorDictModule(\n",
    "    policy_net, in_keys=[\"observation\"], out_keys=[\"logits\"]\n",
    ")\n",
    "\n",
    "from torchrl.modules import ProbabilisticActor, OneHotCategorical\n",
    "\n",
    "policy_module = ProbabilisticActor(\n",
    "    module=policy_module,\n",
    "    spec=env.action_spec,\n",
    "    in_keys=[\"logits\"],\n",
    "    distribution_class=OneHotCategorical,\n",
    "    distribution_kwargs={},\n",
    "    return_log_prob=True,\n",
    "    # we'll need the log-prob for the numerator of the importance weights\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value_Network(nn.Module):\n",
    "    def __init__(self, num_cells) -> None:\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(input_size, num_cells),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_cells, num_cells),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_cells, 1),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        result = self.main(inputs)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.modules import ValueOperator\n",
    "\n",
    "value_net = Value_Network(num_cells).to(device)\n",
    "\n",
    "value_module = ValueOperator(\n",
    "    module=value_net,\n",
    "    in_keys=[\"observation\"],\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tesitng Policy and Value Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running policy:\", policy_module(env.reset().to(device)))\n",
    "print(\"Running value:\", value_module(env.reset().to(device)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.collectors import SyncDataCollector\n",
    "\n",
    "collector = SyncDataCollector(\n",
    "    env,\n",
    "    policy_module,\n",
    "    frames_per_batch=batch_size,\n",
    "    total_frames=total_frames,\n",
    "    split_trajs=True, # Changed\n",
    "    device=device,\n",
    "    reset_at_each_iter=True\n",
    "    # reset_at_each_iter=True, # ??? VERIFY - Should be false (default)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.data.replay_buffers import ReplayBuffer\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "\n",
    "replay_buffer = ReplayBuffer(\n",
    "    storage=LazyTensorStorage(max_size=buffer_size),\n",
    "    sampler=SamplerWithoutReplacement(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.objectives.value import GAE\n",
    "\n",
    "# GAE - Generalized Advantage Estimation\n",
    "advantage_module = GAE(gamma=gamma, lmbda=lmbda, value_network=value_module)\n",
    "\n",
    "\n",
    "from torchrl.objectives import ClipPPOLoss\n",
    "loss_module = ClipPPOLoss(\n",
    "    actor_network=policy_module,\n",
    "    critic_network=value_module,\n",
    "    clip_epsilon=clip_epsilon,\n",
    "    entropy_bonus=bool(entropy_eps),\n",
    "    entropy_coef=entropy_eps,\n",
    "    \n",
    "    critic_coef=critic_coef,\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(loss_module.parameters(), lr)\n",
    "\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "lr_scheduler = LambdaLR(optimizer, lambda epoch: 1 - batch_size / total_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainign Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Policy Device:\", next(policy_module.parameters()).device)\n",
    "print(\"Environment Device:\", env.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.envs.utils import LazyStackedTensorDict\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "logs = defaultdict(list)\n",
    "pbar = tqdm(total=total_frames)\n",
    "eval_str = \"\"\n",
    "\n",
    "# We iterate over the collector until it reaches the total number of frames it was\n",
    "# designed to collect:\n",
    "for i, tensordict_data in enumerate(collector):\n",
    "    # we now have a batch of data to work with. Let's learn something from it.\n",
    "\n",
    "    for _ in range(num_epochs):\n",
    "        # We'll need an \"advantage\" signal to make PPO work.\n",
    "        # We re-compute it at each epoch as its value depends on the value\n",
    "        # network which is updated in the inner loop.\n",
    "\n",
    "        # Compute advantage\n",
    "        advantage_module(tensordict_data)\n",
    "        \n",
    "        data_view = tensordict_data.reshape(-1)\n",
    "        replay_buffer.extend(data_view.cpu())\n",
    "        for _ in range(batch_size // sub_batch_size):\n",
    "          \n",
    "            subdata = replay_buffer.sample(sub_batch_size)\n",
    "            \n",
    "            loss_dict = loss_module(subdata.to(device))\n",
    "            \n",
    "            if 'loss_entropy' in loss_dict:\n",
    "                loss_value = (\n",
    "                    loss_dict[\"loss_objective\"]\n",
    "                    + loss_dict[\"loss_critic\"]\n",
    "                    + loss_dict[\"loss_entropy\"]\n",
    "                )\n",
    "            else:\n",
    "                loss_value = (\n",
    "                    loss_dict[\"loss_objective\"]\n",
    "                    + loss_dict[\"loss_critic\"]\n",
    "                )\n",
    "            \n",
    "            # Optimization: backward, grad clipping and optimization step\n",
    "            loss_value.backward()\n",
    "            \n",
    "            # this is not strictly mandatory but it's good practice to keep\n",
    "            # your gradient norm bounded\n",
    "            torch.nn.utils.clip_grad_norm_(loss_module.parameters(), max_grad_norm)\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    if i % 1 == 0:\n",
    "        # Loggs ---\n",
    "        \n",
    "        # Convert to CPU & NumPy for easier aggregation\n",
    "        rewards = tensordict_data[\"next\", \"reward\"].cpu().numpy().flatten()\n",
    "        done_flags = tensordict_data[\"done\"].cpu().numpy().flatten()\n",
    "        traj_ids = tensordict_data[\"collector\", \"traj_ids\"].cpu().numpy().flatten()\n",
    "\n",
    "        # Create a DataFrame for easier aggregation\n",
    "        df = pd.DataFrame({\"traj_id\": traj_ids, \"reward\": rewards, \"done\": done_flags})\n",
    "\n",
    "        # Sum rewards for each unique trajectory ID\n",
    "        episode_rewards = df.groupby(\"traj_id\")[\"reward\"].sum().to_numpy()\n",
    "    \n",
    "        logs[\"reward\"].append(tensordict_data[\"next\", \"reward\"].mean().item())\n",
    "        logs[\"reward_episodes\"].append(episode_rewards.mean())\n",
    "        \n",
    "        reward_str = (\n",
    "            f\"avg reward={logs['reward'][-1]} (init={logs['reward'][0]}) episode reward={logs['reward_episodes'][-1]} (init={logs['reward_episodes'][0]})\"\n",
    "        )\n",
    "        \n",
    "        logs[\"step_count\"].append(tensordict_data[\"step_count\"].max().item())\n",
    "        stepcount_str = f\"step count (max): {logs['step_count'][-1]}\"\n",
    "        \n",
    "        logs[\"lr\"].append(optimizer.param_groups[0][\"lr\"])\n",
    "        lr_str = f\"lr policy: {logs['lr'][-1]: 4.4f}\"\n",
    "        \n",
    "    pbar.set_description(\", \".join([reward_str, stepcount_str, lr_str]))\n",
    "    pbar.update(tensordict_data.numel())\n",
    "    \n",
    "    # We're also using a learning rate scheduler. Like the gradient clipping,\n",
    "    # this is a nice-to-have but nothing necessary for PPO to work.\n",
    "    lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_spec.space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(logs[\"reward\"])\n",
    "plt.title(\"training rewards (average)\")\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(logs[\"step_count\"])\n",
    "plt.title(\"Max step count (training)\")\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(logs[\"reward_episodes\"])\n",
    "plt.title(\"Reward per episode\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
