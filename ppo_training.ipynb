{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "from torch import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_fork = multiprocessing.get_start_method() == \"fork\"\n",
    "\n",
    "device = (\n",
    "    torch.device(0)\n",
    "    if torch.cuda.is_available() and not is_fork\n",
    "    else torch.device(\"cpu\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load unity environment using `mlagents_envs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlagents_envs.side_channel.engine_configuration_channel import EngineConfigurationChannel\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "\n",
    "channel = EngineConfigurationChannel()\n",
    "env_path = \"C:/Users/Pawel/Documents/Unity_Project/warehouse-bot-training/environment_builds/warehouse_step1_full/Warehouse_Bot.exe\"\n",
    "\n",
    "unity_env = UnityEnvironment(\n",
    "  file_name=env_path,\n",
    "  side_channels=[channel],\n",
    "  additional_args=[\"-batchmode\", \"-nographics\"]\n",
    ")\n",
    "channel.set_configuration_parameters(time_scale=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform environment from `mlagents` to `gymnasium`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gym.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env_gymnasium_wrapper import UnityGymWrapper\n",
    "\n",
    "gymnasium_env = UnityGymWrapper(unity_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating stable_baselines3 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Building onw network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "# # from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "# from stable_baselines3 import PPO\n",
    "\n",
    "# class CustomActorCriticPolicy(ActorCriticPolicy):\n",
    "#     def __init__(self, observation_space, action_space, lr_schedule, *args, **kwargs):\n",
    "#         super(CustomActorCriticPolicy, self).__init__(observation_space, action_space, lr_schedule, *args, **kwargs)\n",
    "\n",
    "#         print(self.features_extractor.features_dim, action_space)\n",
    "        \n",
    "#         # Define a custom shared feature extractor\n",
    "#         self.shared_net = nn.Sequential(\n",
    "#             nn.Linear(self.features_extractor.features_dim, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(256, 128),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "        \n",
    "#         self.policy_net = nn.Sequential(\n",
    "#             nn.Linear(feature_dim, last_layer_dim_pi), nn.ReLU()\n",
    "#         )\n",
    "#         # Value network\n",
    "#         self.value_net = nn.Sequential(\n",
    "#             nn.Linear(feature_dim, last_layer_dim_vf), nn.ReLU()\n",
    "#         )\n",
    "\n",
    "#         print(f\"After override, value_net: {self.value_net}\")  # Debugging to check the size again\n",
    "\n",
    "#     def forward(self, obs, deterministic=False):\n",
    "#         features = self.extract_features(obs)\n",
    "#         print(f\"Extracted features shape: {features.shape}\")  # Check the extracted feature shape\n",
    "\n",
    "#         features = self.shared_net(features)\n",
    "#         print(f\"After shared_net, features shape: {features.shape}\")  # Should be (batch_size, 128)\n",
    "\n",
    "#         action_logits = self.policy_net(features)\n",
    "\n",
    "#         print(f\"Value net input shape (before passing to value_net): {features.shape}\")  # Must be (batch_size, 128)\n",
    "#         value = self.value_net(features)  # Should be fine if features.shape[1] == 128\n",
    "\n",
    "#         return action_logits, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on https://stable-baselines3.readthedocs.io/en/v1.0/guide/custom_policy.html\n",
    "\n",
    "# from typing import Callable, Dict, List, Optional, Tuple, Type, Union\n",
    "\n",
    "# import torch as th\n",
    "# from torch import nn\n",
    "\n",
    "# from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "\n",
    "# class CustomNetwork(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Custom network for policy and value function.\n",
    "#     It receives as input the features extracted by the feature extractor.\n",
    "\n",
    "#     :param feature_dim: dimension of the features extracted with the features_extractor (e.g. features from a CNN)\n",
    "#     :param last_layer_dim_pi: (int) number of units for the last layer of the policy network\n",
    "#     :param last_layer_dim_vf: (int) number of units for the last layer of the value network\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         feature_dim: int,\n",
    "#         last_layer_dim_pi: int = 64,\n",
    "#         last_layer_dim_vf: int = 64,\n",
    "#     ):\n",
    "#         super(CustomNetwork, self).__init__()\n",
    "\n",
    "#         # IMPORTANT:\n",
    "#         # Save output dimensions, used to create the distributions\n",
    "#         self.latent_dim_pi = last_layer_dim_pi\n",
    "#         self.latent_dim_vf = last_layer_dim_vf\n",
    "\n",
    "#         # Policy network\n",
    "#         self.policy_net = nn.Sequential(\n",
    "#             nn.Linear(feature_dim, 128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(128, last_layer_dim_pi),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "#         # Value network\n",
    "#         self.value_net = nn.Sequential(\n",
    "#             nn.Linear(feature_dim, 64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(64, last_layer_dim_vf),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, features: th.Tensor) -> Tuple[th.Tensor, th.Tensor]:\n",
    "#         \"\"\"\n",
    "#         :return: (th.Tensor, th.Tensor) latent_policy, latent_value of the specified network.\n",
    "#             If all layers are shared, then ``latent_policy == latent_value``\n",
    "#         \"\"\"\n",
    "#         return self.policy_net(features), self.value_net(features)\n",
    "\n",
    "\n",
    "# class CustomActorCriticPolicy(ActorCriticPolicy):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         observation_space: gym.spaces.Space,\n",
    "#         action_space: gym.spaces.Space,\n",
    "#         lr_schedule: Callable[[float], float],\n",
    "#         net_arch: Optional[List[Union[int, Dict[str, List[int]]]]] = None,\n",
    "#         activation_fn: Type[nn.Module] = nn.Tanh,\n",
    "#         *args,\n",
    "#         **kwargs,\n",
    "#     ):\n",
    "\n",
    "#         super(CustomActorCriticPolicy, self).__init__(\n",
    "#             observation_space,\n",
    "#             action_space,\n",
    "#             lr_schedule,\n",
    "#             net_arch,\n",
    "#             activation_fn,\n",
    "#             # Pass remaining arguments to base class\n",
    "#             *args,\n",
    "#             **kwargs,\n",
    "#         )\n",
    "#         # Disable orthogonal initialization\n",
    "#         self.ortho_init = False\n",
    "\n",
    "#     def _build_mlp_extractor(self) -> None:\n",
    "#         self.mlp_extractor = CustomNetwork(self.features_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO Hyperparams from mlagents-learn config file\n",
    "\n",
    "```yaml\n",
    "behaviors:\n",
    "  Dlivery_Bot_2:\n",
    "    trainer_type: ppo\n",
    "    hyperparameters:\n",
    "      batch_size: 512\n",
    "      buffer_size: 2560\n",
    "      learning_rate: 0.0003\n",
    "      beta: 0.005\n",
    "      epsilon: 0.2\n",
    "      lambd: 0.95\n",
    "      num_epoch: 3\n",
    "      learning_rate_schedule: linear\n",
    "    network_settings:\n",
    "      normalize: True\n",
    "      hidden_units: 256\n",
    "      num_layers: 2\n",
    "      vis_encode_type: simple\n",
    "    reward_signals:\n",
    "      extrinsic:\n",
    "        gamma: 0.99\n",
    "        strength: 1.0\n",
    "    keep_checkpoints: 5\n",
    "    checkpoint_interval: 100000\n",
    "    max_steps: 4000000\n",
    "    time_horizon: 1024\n",
    "    summary_freq: 10000\n",
    "    # threaded: False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "import torch.nn as nn\n",
    "\n",
    "# model = PPO(\"MlpPolicy\", gymnasium_env, verbose=1,\n",
    "#             learning_rate=3e-4,\n",
    "#             n_steps=10240,\n",
    "#             batch_size=512,\n",
    "#             n_epochs=8,\n",
    "#             clip_range=0.2,\n",
    "#             gamma=0.995,\n",
    "#             gae_lambda=0.96,\n",
    "#             seed=0,\n",
    "#             ent_coef=0.005,\n",
    "#             vf_coef=0.5,\n",
    "#             policy_kwargs={\n",
    "#               \"net_arch\": [dict(pi=[128, 64], vf=[64, 32])],\n",
    "#               \"activation_fn\": nn.ReLU\n",
    "#             }\n",
    "# )\n",
    "\n",
    "\n",
    "model = PPO.load('./saved_models/warehouse_step1_full_2_2', gymnasium_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=500_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./saved_models/warehouse_step1_full_2_3')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
