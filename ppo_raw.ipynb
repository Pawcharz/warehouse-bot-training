{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch as th\n",
    "from torch import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_fork = multiprocessing.get_start_method() == \"fork\"\n",
    "\n",
    "device = (\n",
    "    th.device(0)\n",
    "    if th.cuda.is_available() and not is_fork\n",
    "    else th.device(\"cpu\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform environment from `mlagents` to `gymnasium`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.1\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "print(gym.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlagents_envs.side_channel.engine_configuration_channel import EngineConfigurationChannel\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "\n",
    "from env_camera_raycasts_gymnasium_wrapper import UnityCameraRaycastsGymWrapper\n",
    "\n",
    "env_path = \"D:/_Thesis/warehouse-bot-training/environment_builds/warehouse_stage2_find/Warehouse_Bot.exe\"\n",
    "def make_env():\n",
    "\n",
    "  channel = EngineConfigurationChannel()\n",
    "\n",
    "  unity_env = UnityEnvironment(\n",
    "    file_name=env_path,\n",
    "    side_channels=[channel],\n",
    "  )\n",
    "  \n",
    "  channel.set_configuration_parameters(time_scale=1)\n",
    "  \n",
    "  gymnasium_env = UnityCameraRaycastsGymWrapper(unity_env)\n",
    "  \n",
    "  print(gymnasium_env.observation_space)\n",
    "  \n",
    "  return gymnasium_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# Actor-Critic Network\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super().__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.policy_head = nn.Linear(64, act_dim)\n",
    "        self.value_head = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.shared(x)\n",
    "        return self.policy_head(x), self.value_head(x)\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        logits, value = self.forward(obs)\n",
    "        dist = Categorical(logits=logits)\n",
    "        action = dist.sample()\n",
    "        return action, dist.log_prob(action), dist.entropy(), value.squeeze()\n",
    "\n",
    "    def evaluate_actions(self, obs, actions):\n",
    "        logits, values = self.forward(obs)\n",
    "        dist = Categorical(logits=logits)\n",
    "        log_probs = dist.log_prob(actions)\n",
    "        entropy = dist.entropy()\n",
    "        return log_probs, entropy, values.squeeze()\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * th.sigmoid(x)\n",
    "\n",
    "class ActorCriticMultimodal(nn.Module):\n",
    "    def __init__(self, act_dim, visual_size=[3, 36, 64], vector_obs_size=128):\n",
    "        super().__init__()\n",
    "        bands = visual_size[0]\n",
    "\n",
    "        # Shapes of image and vector inputs: [<batch size>, <bands, height, width>], [<batch size>, <length>]\n",
    "\n",
    "        visual_out_size = 64\n",
    "        vector_out_size = 32\n",
    "\n",
    "        # Visual Encoder\n",
    "        self.visual_encoder_cnn = nn.Sequential(\n",
    "            nn.Conv2d(bands, 16, kernel_size=5, stride=4, padding=0),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        # Compute flattened visual output size from dummy input\n",
    "        dummy_input = th.zeros(1, bands, visual_size[1], visual_size[2])\n",
    "        with th.no_grad():\n",
    "            visual_encoder_cnn_out_size = self.visual_encoder_cnn(dummy_input).shape[1]\n",
    "\n",
    "        self.visual_encoder_mlp = nn.Sequential(\n",
    "            nn.Linear(visual_encoder_cnn_out_size, 64),\n",
    "            Swish(),\n",
    "            nn.Linear(64, visual_out_size),\n",
    "            Swish()\n",
    "        )\n",
    "        \n",
    "        # Vector Encoder\n",
    "        self.vector_encoder = nn.Sequential(\n",
    "            nn.Linear(vector_obs_size, 32),\n",
    "            Swish(),\n",
    "            nn.Linear(32, vector_out_size),                             \n",
    "            Swish()\n",
    "        )\n",
    "\n",
    "        # Concatenation Network\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(visual_out_size + vector_out_size, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.policy_head = nn.Linear(32, act_dim)\n",
    "        self.value_head = nn.Linear(32, 1)\n",
    "\n",
    "    \n",
    "    def forward(self, observations):\n",
    "        image = observations[\"image\"].float()\n",
    "        vector = observations[\"vector\"]\n",
    "\n",
    "        image_features = self.visual_encoder_cnn(image)\n",
    "        image_features = self.visual_encoder_mlp(image_features)\n",
    "        vector_features = self.vector_encoder(vector)\n",
    "\n",
    "        combined = th.cat([image_features, vector_features], dim=1)\n",
    "        x = self.shared(combined)\n",
    "        return self.policy_head(x), self.value_head(x)\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        logits, value = self.forward(obs)\n",
    "        dist = Categorical(logits=logits)\n",
    "\n",
    "        # Debug: Check action distribution\n",
    "        probs = dist.probs\n",
    "        # print(f\"Action probabilities: {probs}\")\n",
    "        # print(f\"Logits range: {logits.min():.3f} to {logits.max():.3f}\")\n",
    "        # print(dist.entropy())\n",
    "\n",
    "        action = dist.sample()\n",
    "        return action, dist.log_prob(action), dist.entropy(), value.squeeze()\n",
    "\n",
    "    def evaluate_actions(self, obs, actions):\n",
    "        logits, values = self.forward(obs)\n",
    "        dist = Categorical(logits=logits)\n",
    "        log_probs = dist.log_prob(actions)\n",
    "        entropy = dist.entropy()\n",
    "        return log_probs, entropy, values.squeeze()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"\n",
    "    Count parameters in each block of the network and total parameters.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing parameter counts for each block and total\n",
    "    \"\"\"\n",
    "    total_params = 0\n",
    "    block_params = {}\n",
    "    \n",
    "    for name, module in model.named_children():\n",
    "        params = sum(p.numel() for p in module.parameters())\n",
    "        block_params[name] = params\n",
    "        total_params += params\n",
    "        \n",
    "    block_params['total'] = total_params\n",
    "    return block_params\n",
    "\n",
    "# Example usage:\n",
    "# param_counts = count_parameters(model)\n",
    "# print(\"Parameter counts by block:\")\n",
    "# for block, count in param_counts.items():\n",
    "#     print(f\"{block}: {count:,} parameters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create settings dictionary\n",
    "settings = {\n",
    "    'gamma': 0.99,\n",
    "    'lam': 0.95,\n",
    "    'clip_eps': 0.2,\n",
    "    'ppo_epochs': 4,\n",
    "    'batch_size': 64,\n",
    "    'update_timesteps': 2048,\n",
    "    'lr': 1e-4,\n",
    "    'val_loss_coef': 0.5,\n",
    "    'ent_loss_coef':  0.01,\n",
    "    'device': th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict('image': Box(0, 255, (3, 36, 64), uint8), 'vector': Box(0.0, 255.0, (80,), float32))\n"
     ]
    }
   ],
   "source": [
    "# Create environment\n",
    "env = make_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'visual_encoder_cnn': 15104, 'visual_encoder_mlp': 20608, 'vector_encoder': 3648, 'shared': 8288, 'policy_head': 99, 'value_head': 33, 'total': 47780}\n"
     ]
    }
   ],
   "source": [
    "act_dim = env.action_space.n\n",
    "\n",
    "model_net = ActorCriticMultimodal(act_dim, visual_size=[3, 36, 64], vector_obs_size=80)\n",
    "param_counts = count_parameters(model_net)\n",
    "print(param_counts)\n",
    "\n",
    "from PPO_algorithm import PPOAgent\n",
    "agent = PPOAgent(model_net, settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original advantages - mean: -12.0037, std: 19.9113\n",
      "Normalized advantages - mean: -0.0000, std: 19.9113\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0881, std: 0.1109\n",
      "Log probs new - mean: -1.0881, std: 0.1109\n",
      "Log prob diff - mean: 0.0000, std: 0.0000\n",
      "Ratios - mean: 1.0000, std: 0.0000\n",
      "Advantages - mean: 2.2868, std: 20.8037\n",
      "Surr1 - mean: 2.2868, std: 20.8037\n",
      "Surr2 - mean: 2.2868, std: 20.8037\n",
      "Policy loss: -2.286848\n",
      "Value loss: 260.224396\n",
      "Entropy bonus: 0.010924\n",
      "⚠️  WARNING: Log probabilities barely changed\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0941, std: 0.1179\n",
      "Log probs new - mean: -1.0941, std: 0.1180\n",
      "Log prob diff - mean: -0.0001, std: 0.0003\n",
      "Ratios - mean: 0.9999, std: 0.0003\n",
      "Advantages - mean: 3.3541, std: 25.3181\n",
      "Surr1 - mean: 3.3537, std: 25.3164\n",
      "Surr2 - mean: 3.3537, std: 25.3164\n",
      "Policy loss: -3.353663\n",
      "Value loss: 352.866882\n",
      "Entropy bonus: 0.010924\n",
      "⚠️  WARNING: Log probabilities barely changed\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.1095, std: 0.1096\n",
      "Log probs new - mean: -1.1094, std: 0.1093\n",
      "Log prob diff - mean: 0.0001, std: 0.0007\n",
      "Ratios - mean: 1.0001, std: 0.0007\n",
      "Advantages - mean: -0.5345, std: 15.8015\n",
      "Surr1 - mean: -0.5329, std: 15.8100\n",
      "Surr2 - mean: -0.5329, std: 15.8100\n",
      "Policy loss: 0.532913\n",
      "Value loss: 201.390213\n",
      "Entropy bonus: 0.010925\n",
      "⚠️  WARNING: Log probabilities barely changed\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.1211, std: 0.1036\n",
      "Log probs new - mean: -1.1208, std: 0.1029\n",
      "Log prob diff - mean: 0.0003, std: 0.0014\n",
      "Ratios - mean: 1.0003, std: 0.0014\n",
      "Advantages - mean: 2.7973, std: 21.8152\n",
      "Surr1 - mean: 2.8057, std: 21.8422\n",
      "Surr2 - mean: 2.8057, std: 21.8422\n",
      "Policy loss: -2.805735\n",
      "Value loss: 276.494934\n",
      "Entropy bonus: 0.010926\n",
      "⚠️  WARNING: Log probabilities barely changed\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0933, std: 0.1146\n",
      "Log probs new - mean: -1.0934, std: 0.1131\n",
      "Log prob diff - mean: -0.0001, std: 0.0025\n",
      "Ratios - mean: 0.9999, std: 0.0025\n",
      "Advantages - mean: 1.4797, std: 25.1580\n",
      "Surr1 - mean: 1.4659, std: 25.1104\n",
      "Surr2 - mean: 1.4659, std: 25.1104\n",
      "Policy loss: -1.465860\n",
      "Value loss: 366.715027\n",
      "Entropy bonus: 0.010926\n",
      "⚠️  WARNING: Log probabilities barely changed\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.1044, std: 0.1129\n",
      "Log probs new - mean: -1.1044, std: 0.1113\n",
      "Log prob diff - mean: 0.0001, std: 0.0031\n",
      "Ratios - mean: 1.0001, std: 0.0031\n",
      "Advantages - mean: -1.3066, std: 17.1462\n",
      "Surr1 - mean: -1.3111, std: 17.1519\n",
      "Surr2 - mean: -1.3111, std: 17.1519\n",
      "Policy loss: 1.311140\n",
      "Value loss: 232.998962\n",
      "Entropy bonus: 0.010926\n",
      "⚠️  WARNING: Log probabilities barely changed\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.1015, std: 0.1144\n",
      "Log probs new - mean: -1.1016, std: 0.1123\n",
      "Log prob diff - mean: -0.0001, std: 0.0036\n",
      "Ratios - mean: 0.9999, std: 0.0036\n",
      "Advantages - mean: -0.5849, std: 15.5757\n",
      "Surr1 - mean: -0.5932, std: 15.6017\n",
      "Surr2 - mean: -0.5932, std: 15.6017\n",
      "Policy loss: 0.593162\n",
      "Value loss: 198.325775\n",
      "Entropy bonus: 0.010927\n",
      "⚠️  WARNING: Log probabilities barely changed\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0925, std: 0.1106\n",
      "Log probs new - mean: -1.0924, std: 0.1079\n",
      "Log prob diff - mean: 0.0001, std: 0.0044\n",
      "Ratios - mean: 1.0001, std: 0.0044\n",
      "Advantages - mean: -2.1751, std: 20.6925\n",
      "Surr1 - mean: -2.1765, std: 20.7175\n",
      "Surr2 - mean: -2.1765, std: 20.7175\n",
      "Policy loss: 2.176473\n",
      "Value loss: 310.851349\n",
      "Entropy bonus: 0.010927\n",
      "⚠️  WARNING: Log probabilities barely changed\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0836, std: 0.1129\n",
      "Log probs new - mean: -1.0838, std: 0.1094\n",
      "Log prob diff - mean: -0.0003, std: 0.0050\n",
      "Ratios - mean: 0.9997, std: 0.0050\n",
      "Advantages - mean: 1.5085, std: 24.6706\n",
      "Surr1 - mean: 1.5142, std: 24.6173\n",
      "Surr2 - mean: 1.5142, std: 24.6173\n",
      "Policy loss: -1.514207\n",
      "Value loss: 354.288208\n",
      "Entropy bonus: 0.010928\n",
      "⚠️  WARNING: Log probabilities barely changed\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.1033, std: 0.1091\n",
      "Log probs new - mean: -1.1027, std: 0.1054\n",
      "Log prob diff - mean: 0.0006, std: 0.0056\n",
      "Ratios - mean: 1.0006, std: 0.0056\n",
      "Advantages - mean: -1.0693, std: 15.7396\n",
      "Surr1 - mean: -1.0692, std: 15.7509\n",
      "Surr2 - mean: -1.0692, std: 15.7509\n",
      "Policy loss: 1.069189\n",
      "Value loss: 206.898178\n",
      "Entropy bonus: 0.010929\n",
      "⚠️  WARNING: Log probabilities barely changed\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.1224, std: 0.1112\n",
      "Log probs new - mean: -1.1215, std: 0.1075\n",
      "Log prob diff - mean: 0.0009, std: 0.0058\n",
      "Ratios - mean: 1.0009, std: 0.0058\n",
      "Advantages - mean: 0.0671, std: 22.4074\n",
      "Surr1 - mean: 0.1026, std: 22.4918\n",
      "Surr2 - mean: 0.1026, std: 22.4918\n",
      "Policy loss: -0.102602\n",
      "Value loss: 317.872925\n",
      "Entropy bonus: 0.010929\n",
      "⚠️  WARNING: Log probabilities barely changed\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0898, std: 0.1119\n",
      "Log probs new - mean: -1.0899, std: 0.1064\n",
      "Log prob diff - mean: -0.0000, std: 0.0074\n",
      "Ratios - mean: 1.0000, std: 0.0074\n",
      "Advantages - mean: 1.0156, std: 21.1692\n",
      "Surr1 - mean: 0.9993, std: 21.0493\n",
      "Surr2 - mean: 0.9993, std: 21.0493\n",
      "Policy loss: -0.999321\n",
      "Value loss: 280.428955\n",
      "Entropy bonus: 0.010930\n",
      "⚠️  WARNING: Log probabilities barely changed\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0984, std: 0.1082\n",
      "Log probs new - mean: -1.0977, std: 0.1024\n",
      "Log prob diff - mean: 0.0007, std: 0.0081\n",
      "Ratios - mean: 1.0007, std: 0.0081\n",
      "Advantages - mean: 0.4299, std: 16.7773\n",
      "Surr1 - mean: 0.4438, std: 16.8595\n",
      "Surr2 - mean: 0.4438, std: 16.8595\n",
      "Policy loss: -0.443799\n",
      "Value loss: 204.929199\n",
      "Entropy bonus: 0.010931\n",
      "⚠️  WARNING: Log probabilities barely changed\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0904, std: 0.1055\n",
      "Log probs new - mean: -1.0896, std: 0.0986\n",
      "Log prob diff - mean: 0.0009, std: 0.0092\n",
      "Ratios - mean: 1.0009, std: 0.0092\n",
      "Advantages - mean: -2.0769, std: 15.9119\n",
      "Surr1 - mean: -2.0686, std: 15.9317\n",
      "Surr2 - mean: -2.0686, std: 15.9317\n",
      "Policy loss: 2.068559\n",
      "Value loss: 222.979431\n",
      "Entropy bonus: 0.010932\n",
      "⚠️  WARNING: Log probabilities barely changed\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0884, std: 0.1165\n",
      "Log probs new - mean: -1.0896, std: 0.1090\n",
      "Log prob diff - mean: -0.0012, std: 0.0098\n",
      "Ratios - mean: 0.9989, std: 0.0098\n",
      "Advantages - mean: 0.8717, std: 23.1609\n",
      "Surr1 - mean: 0.8336, std: 22.9913\n",
      "Surr2 - mean: 0.8336, std: 22.9913\n",
      "Policy loss: -0.833604\n",
      "Value loss: 325.329163\n",
      "Entropy bonus: 0.010932\n",
      "⚠️  WARNING: Log probabilities barely changed\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.1074, std: 0.1154\n",
      "Log probs new - mean: -1.1072, std: 0.1082\n",
      "Log prob diff - mean: 0.0002, std: 0.0100\n",
      "Ratios - mean: 1.0003, std: 0.0100\n",
      "Advantages - mean: -1.2828, std: 14.4404\n",
      "Surr1 - mean: -1.2884, std: 14.4656\n",
      "Surr2 - mean: -1.2884, std: 14.4656\n",
      "Policy loss: 1.288399\n",
      "Value loss: 190.042984\n",
      "Entropy bonus: 0.010932\n",
      "⚠️  WARNING: Log probabilities barely changed\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0899, std: 0.1119\n",
      "Log probs new - mean: -1.0900, std: 0.1035\n",
      "Log prob diff - mean: -0.0002, std: 0.0112\n",
      "Ratios - mean: 0.9999, std: 0.0112\n",
      "Advantages - mean: -1.1133, std: 20.1306\n",
      "Surr1 - mean: -1.1610, std: 19.9963\n",
      "Surr2 - mean: -1.1610, std: 19.9963\n",
      "Policy loss: 1.161047\n",
      "Value loss: 284.566498\n",
      "Entropy bonus: 0.010933\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0878, std: 0.1109\n",
      "Log probs new - mean: -1.0877, std: 0.1022\n",
      "Log prob diff - mean: 0.0001, std: 0.0119\n",
      "Ratios - mean: 1.0002, std: 0.0119\n",
      "Advantages - mean: -1.2796, std: 19.3969\n",
      "Surr1 - mean: -1.2783, std: 19.4489\n",
      "Surr2 - mean: -1.2783, std: 19.4489\n",
      "Policy loss: 1.278328\n",
      "Value loss: 272.447876\n",
      "Entropy bonus: 0.010933\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0988, std: 0.1102\n",
      "Log probs new - mean: -1.0977, std: 0.1018\n",
      "Log prob diff - mean: 0.0011, std: 0.0122\n",
      "Ratios - mean: 1.0012, std: 0.0122\n",
      "Advantages - mean: 0.4692, std: 23.0477\n",
      "Surr1 - mean: 0.4971, std: 23.2149\n",
      "Surr2 - mean: 0.4971, std: 23.2149\n",
      "Policy loss: -0.497138\n",
      "Value loss: 327.066620\n",
      "Entropy bonus: 0.010933\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0945, std: 0.0995\n",
      "Log probs new - mean: -1.0914, std: 0.0902\n",
      "Log prob diff - mean: 0.0031, std: 0.0134\n",
      "Ratios - mean: 1.0032, std: 0.0134\n",
      "Advantages - mean: -3.1958, std: 17.8767\n",
      "Surr1 - mean: -3.1668, std: 17.9945\n",
      "Surr2 - mean: -3.1668, std: 17.9945\n",
      "Policy loss: 3.166777\n",
      "Value loss: 271.558990\n",
      "Entropy bonus: 0.010934\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.1126, std: 0.1118\n",
      "Log probs new - mean: -1.1114, std: 0.1031\n",
      "Log prob diff - mean: 0.0011, std: 0.0131\n",
      "Ratios - mean: 1.0012, std: 0.0130\n",
      "Advantages - mean: -5.5151, std: 14.6320\n",
      "Surr1 - mean: -5.5013, std: 14.6501\n",
      "Surr2 - mean: -5.5013, std: 14.6501\n",
      "Policy loss: 5.501321\n",
      "Value loss: 257.300049\n",
      "Entropy bonus: 0.010935\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0673, std: 0.1116\n",
      "Log probs new - mean: -1.0697, std: 0.1000\n",
      "Log prob diff - mean: -0.0024, std: 0.0154\n",
      "Ratios - mean: 0.9977, std: 0.0154\n",
      "Advantages - mean: 4.3981, std: 23.9915\n",
      "Surr1 - mean: 4.3827, std: 23.7498\n",
      "Surr2 - mean: 4.3827, std: 23.7498\n",
      "Policy loss: -4.382714\n",
      "Value loss: 311.487671\n",
      "Entropy bonus: 0.010935\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0815, std: 0.1168\n",
      "Log probs new - mean: -1.0844, std: 0.1052\n",
      "Log prob diff - mean: -0.0029, std: 0.0157\n",
      "Ratios - mean: 0.9972, std: 0.0156\n",
      "Advantages - mean: 0.1237, std: 14.6984\n",
      "Surr1 - mean: 0.1581, std: 14.6783\n",
      "Surr2 - mean: 0.1581, std: 14.6783\n",
      "Policy loss: -0.158087\n",
      "Value loss: 175.730530\n",
      "Entropy bonus: 0.010936\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.1078, std: 0.1088\n",
      "Log probs new - mean: -1.1058, std: 0.0984\n",
      "Log prob diff - mean: 0.0020, std: 0.0161\n",
      "Ratios - mean: 1.0022, std: 0.0161\n",
      "Advantages - mean: 0.4992, std: 19.6607\n",
      "Surr1 - mean: 0.5129, std: 19.7419\n",
      "Surr2 - mean: 0.5129, std: 19.7419\n",
      "Policy loss: -0.512902\n",
      "Value loss: 255.255173\n",
      "Entropy bonus: 0.010936\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0942, std: 0.1116\n",
      "Log probs new - mean: -1.0941, std: 0.0995\n",
      "Log prob diff - mean: 0.0001, std: 0.0176\n",
      "Ratios - mean: 1.0002, std: 0.0176\n",
      "Advantages - mean: -0.0034, std: 16.4781\n",
      "Surr1 - mean: -0.0563, std: 16.2973\n",
      "Surr2 - mean: -0.0563, std: 16.2973\n",
      "Policy loss: 0.056343\n",
      "Value loss: 204.419937\n",
      "Entropy bonus: 0.010938\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0859, std: 0.1052\n",
      "Log probs new - mean: -1.0845, std: 0.0915\n",
      "Log prob diff - mean: 0.0013, std: 0.0194\n",
      "Ratios - mean: 1.0015, std: 0.0194\n",
      "Advantages - mean: 2.0377, std: 23.6154\n",
      "Surr1 - mean: 2.0196, std: 23.5342\n",
      "Surr2 - mean: 2.0196, std: 23.5342\n",
      "Policy loss: -2.019602\n",
      "Value loss: 323.036072\n",
      "Entropy bonus: 0.010938\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.1099, std: 0.1069\n",
      "Log probs new - mean: -1.1068, std: 0.0953\n",
      "Log prob diff - mean: 0.0031, std: 0.0187\n",
      "Ratios - mean: 1.0033, std: 0.0187\n",
      "Advantages - mean: -0.2856, std: 20.1508\n",
      "Surr1 - mean: -0.2747, std: 20.2170\n",
      "Surr2 - mean: -0.2747, std: 20.2170\n",
      "Policy loss: 0.274750\n",
      "Value loss: 273.880676\n",
      "Entropy bonus: 0.010939\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0657, std: 0.1158\n",
      "Log probs new - mean: -1.0712, std: 0.1005\n",
      "Log prob diff - mean: -0.0056, std: 0.0200\n",
      "Ratios - mean: 0.9946, std: 0.0200\n",
      "Advantages - mean: -1.5248, std: 19.5823\n",
      "Surr1 - mean: -1.4597, std: 19.3060\n",
      "Surr2 - mean: -1.4597, std: 19.3060\n",
      "Policy loss: 1.459744\n",
      "Value loss: 278.560547\n",
      "Entropy bonus: 0.010939\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0958, std: 0.1133\n",
      "Log probs new - mean: -1.0962, std: 0.0987\n",
      "Log prob diff - mean: -0.0004, std: 0.0209\n",
      "Ratios - mean: 0.9998, std: 0.0209\n",
      "Advantages - mean: -0.7307, std: 23.6244\n",
      "Surr1 - mean: -0.6904, std: 23.7213\n",
      "Surr2 - mean: -0.6904, std: 23.7213\n",
      "Policy loss: 0.690382\n",
      "Value loss: 354.054871\n",
      "Entropy bonus: 0.010939\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0926, std: 0.1103\n",
      "Log probs new - mean: -1.0922, std: 0.0946\n",
      "Log prob diff - mean: 0.0005, std: 0.0228\n",
      "Ratios - mean: 1.0007, std: 0.0228\n",
      "Advantages - mean: 2.4533, std: 21.0690\n",
      "Surr1 - mean: 2.4178, std: 20.7648\n",
      "Surr2 - mean: 2.4178, std: 20.7648\n",
      "Policy loss: -2.417779\n",
      "Value loss: 262.758392\n",
      "Entropy bonus: 0.010941\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0925, std: 0.1165\n",
      "Log probs new - mean: -1.0950, std: 0.1001\n",
      "Log prob diff - mean: -0.0025, std: 0.0227\n",
      "Ratios - mean: 0.9978, std: 0.0227\n",
      "Advantages - mean: -1.3514, std: 16.2915\n",
      "Surr1 - mean: -1.3460, std: 16.3185\n",
      "Surr2 - mean: -1.3460, std: 16.3185\n",
      "Policy loss: 1.345968\n",
      "Value loss: 217.896683\n",
      "Entropy bonus: 0.010941\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0934, std: 0.1141\n",
      "Log probs new - mean: -1.0950, std: 0.0970\n",
      "Log prob diff - mean: -0.0015, std: 0.0244\n",
      "Ratios - mean: 0.9988, std: 0.0243\n",
      "Advantages - mean: 1.1251, std: 20.1803\n",
      "Surr1 - mean: 1.1545, std: 20.1832\n",
      "Surr2 - mean: 1.1545, std: 20.1832\n",
      "Policy loss: -1.154456\n",
      "Value loss: 257.978699\n",
      "Entropy bonus: 0.010942\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.1341, std: 0.1008\n",
      "Log probs new - mean: -1.1266, std: 0.0898\n",
      "Log prob diff - mean: 0.0076, std: 0.0221\n",
      "Ratios - mean: 1.0078, std: 0.0221\n",
      "Advantages - mean: -3.7853, std: 14.2488\n",
      "Surr1 - mean: -3.7410, std: 14.2859\n",
      "Surr2 - mean: -3.7410, std: 14.2859\n",
      "Policy loss: 3.740973\n",
      "Value loss: 216.942627\n",
      "Entropy bonus: 0.010942\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0914, std: 0.1245\n",
      "Log probs new - mean: -1.0986, std: 0.1054\n",
      "Log prob diff - mean: -0.0072, std: 0.0246\n",
      "Ratios - mean: 0.9931, std: 0.0244\n",
      "Advantages - mean: 2.8233, std: 24.8517\n",
      "Surr1 - mean: 2.8741, std: 24.9937\n",
      "Surr2 - mean: 2.8741, std: 24.9937\n",
      "Policy loss: -2.874081\n",
      "Value loss: 344.610962\n",
      "Entropy bonus: 0.010943\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.1034, std: 0.1030\n",
      "Log probs new - mean: -1.0975, std: 0.0858\n",
      "Log prob diff - mean: 0.0059, std: 0.0279\n",
      "Ratios - mean: 1.0063, std: 0.0278\n",
      "Advantages - mean: -1.0736, std: 16.4521\n",
      "Surr1 - mean: -1.1083, std: 16.1757\n",
      "Surr2 - mean: -1.1083, std: 16.1757\n",
      "Policy loss: 1.108303\n",
      "Value loss: 216.445374\n",
      "Entropy bonus: 0.010944\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0916, std: 0.1133\n",
      "Log probs new - mean: -1.0927, std: 0.0926\n",
      "Log prob diff - mean: -0.0011, std: 0.0298\n",
      "Ratios - mean: 0.9993, std: 0.0298\n",
      "Advantages - mean: -3.9347, std: 14.2673\n",
      "Surr1 - mean: -3.9668, std: 14.1801\n",
      "Surr2 - mean: -3.9668, std: 14.1801\n",
      "Policy loss: 3.966813\n",
      "Value loss: 224.356567\n",
      "Entropy bonus: 0.010945\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.1055, std: 0.1039\n",
      "Log probs new - mean: -1.0998, std: 0.0846\n",
      "Log prob diff - mean: 0.0056, std: 0.0307\n",
      "Ratios - mean: 1.0061, std: 0.0307\n",
      "Advantages - mean: 3.7021, std: 23.9101\n",
      "Surr1 - mean: 3.6876, std: 23.8869\n",
      "Surr2 - mean: 3.6876, std: 23.8869\n",
      "Policy loss: -3.687582\n",
      "Value loss: 314.320679\n",
      "Entropy bonus: 0.010946\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0849, std: 0.1137\n",
      "Log probs new - mean: -1.0882, std: 0.0901\n",
      "Log prob diff - mean: -0.0033, std: 0.0333\n",
      "Ratios - mean: 0.9972, std: 0.0331\n",
      "Advantages - mean: 2.3412, std: 26.2413\n",
      "Surr1 - mean: 2.3329, std: 25.7898\n",
      "Surr2 - mean: 2.3329, std: 25.7898\n",
      "Policy loss: -2.332938\n",
      "Value loss: 383.752472\n",
      "Entropy bonus: 0.010947\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0925, std: 0.1104\n",
      "Log probs new - mean: -1.0921, std: 0.0874\n",
      "Log prob diff - mean: 0.0005, std: 0.0346\n",
      "Ratios - mean: 1.0010, std: 0.0345\n",
      "Advantages - mean: -2.7189, std: 14.4477\n",
      "Surr1 - mean: -2.6107, std: 14.5504\n",
      "Surr2 - mean: -2.6107, std: 14.5504\n",
      "Policy loss: 2.610674\n",
      "Value loss: 208.107956\n",
      "Entropy bonus: 0.010947\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0926, std: 0.1107\n",
      "Log probs new - mean: -1.0922, std: 0.0869\n",
      "Log prob diff - mean: 0.0004, std: 0.0361\n",
      "Ratios - mean: 1.0011, std: 0.0360\n",
      "Advantages - mean: 1.6804, std: 24.1522\n",
      "Surr1 - mean: 1.8526, std: 24.6558\n",
      "Surr2 - mean: 1.8526, std: 24.6558\n",
      "Policy loss: -1.852592\n",
      "Value loss: 338.208557\n",
      "Entropy bonus: 0.010948\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0933, std: 0.1077\n",
      "Log probs new - mean: -1.0911, std: 0.0833\n",
      "Log prob diff - mean: 0.0021, std: 0.0387\n",
      "Ratios - mean: 1.0029, std: 0.0386\n",
      "Advantages - mean: -1.7921, std: 15.7596\n",
      "Surr1 - mean: -1.7916, std: 15.8995\n",
      "Surr2 - mean: -1.7916, std: 15.8995\n",
      "Policy loss: 1.791643\n",
      "Value loss: 214.341888\n",
      "Entropy bonus: 0.010949\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.1460, std: 0.1014\n",
      "Log probs new - mean: -1.1353, std: 0.0878\n",
      "Log prob diff - mean: 0.0107, std: 0.0317\n",
      "Ratios - mean: 1.0113, std: 0.0317\n",
      "Advantages - mean: -0.1146, std: 17.4877\n",
      "Surr1 - mean: -0.2075, std: 17.4740\n",
      "Surr2 - mean: -0.2075, std: 17.4740\n",
      "Policy loss: 0.207545\n",
      "Value loss: 221.183624\n",
      "Entropy bonus: 0.010949\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0983, std: 0.1061\n",
      "Log probs new - mean: -1.0925, std: 0.0819\n",
      "Log prob diff - mean: 0.0059, std: 0.0415\n",
      "Ratios - mean: 1.0067, std: 0.0414\n",
      "Advantages - mean: 3.4226, std: 24.1073\n",
      "Surr1 - mean: 3.4255, std: 24.1565\n",
      "Surr2 - mean: 3.4255, std: 24.1565\n",
      "Policy loss: -3.425532\n",
      "Value loss: 320.931152\n",
      "Entropy bonus: 0.010949\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0688, std: 0.1090\n",
      "Log probs new - mean: -1.0752, std: 0.0782\n",
      "Log prob diff - mean: -0.0064, std: 0.0453\n",
      "Ratios - mean: 0.9947, std: 0.0452\n",
      "Advantages - mean: -1.3963, std: 19.0865\n",
      "Surr1 - mean: -1.4566, std: 18.4733\n",
      "Surr2 - mean: -1.4566, std: 18.4733\n",
      "Policy loss: 1.456568\n",
      "Value loss: 265.687744\n",
      "Entropy bonus: 0.010950\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0992, std: 0.1094\n",
      "Log probs new - mean: -1.0963, std: 0.0836\n",
      "Log prob diff - mean: 0.0029, std: 0.0454\n",
      "Ratios - mean: 1.0039, std: 0.0453\n",
      "Advantages - mean: -0.6672, std: 13.8573\n",
      "Surr1 - mean: -0.5844, std: 13.8720\n",
      "Surr2 - mean: -0.5844, std: 13.8720\n",
      "Policy loss: 0.584400\n",
      "Value loss: 171.448456\n",
      "Entropy bonus: 0.010951\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0591, std: 0.1047\n",
      "Log probs new - mean: -1.0671, std: 0.0699\n",
      "Log prob diff - mean: -0.0080, std: 0.0511\n",
      "Ratios - mean: 0.9933, std: 0.0510\n",
      "Advantages - mean: -3.4186, std: 18.3436\n",
      "Surr1 - mean: -3.4897, std: 17.8775\n",
      "Surr2 - mean: -3.4897, std: 17.8775\n",
      "Policy loss: 3.489657\n",
      "Value loss: 280.375793\n",
      "Entropy bonus: 0.010951\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0969, std: 0.1169\n",
      "Log probs new - mean: -1.1021, std: 0.0872\n",
      "Log prob diff - mean: -0.0052, std: 0.0473\n",
      "Ratios - mean: 0.9959, std: 0.0470\n",
      "Advantages - mean: 0.9503, std: 21.2465\n",
      "Surr1 - mean: 0.9428, std: 21.4889\n",
      "Surr2 - mean: 0.9428, std: 21.4889\n",
      "Policy loss: -0.942787\n",
      "Value loss: 280.111542\n",
      "Entropy bonus: 0.010951\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0934, std: 0.1145\n",
      "Log probs new - mean: -1.0967, std: 0.0843\n",
      "Log prob diff - mean: -0.0034, std: 0.0516\n",
      "Ratios - mean: 0.9980, std: 0.0514\n",
      "Advantages - mean: 0.3066, std: 15.8986\n",
      "Surr1 - mean: 0.2965, std: 15.5876\n",
      "Surr2 - mean: 0.2965, std: 15.5876\n",
      "Policy loss: -0.296456\n",
      "Value loss: 189.368942\n",
      "Entropy bonus: 0.010952\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.1073, std: 0.1149\n",
      "Log probs new - mean: -1.1084, std: 0.0863\n",
      "Log prob diff - mean: -0.0012, std: 0.0509\n",
      "Ratios - mean: 1.0001, std: 0.0507\n",
      "Advantages - mean: 0.2985, std: 21.6648\n",
      "Surr1 - mean: 0.1177, std: 21.2363\n",
      "Surr2 - mean: 0.1177, std: 21.2363\n",
      "Policy loss: -0.117724\n",
      "Value loss: 295.888428\n",
      "Entropy bonus: 0.010953\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.1017, std: 0.1141\n",
      "Log probs new - mean: -1.1031, std: 0.0840\n",
      "Log prob diff - mean: -0.0015, std: 0.0550\n",
      "Ratios - mean: 1.0000, std: 0.0548\n",
      "Advantages - mean: 0.2144, std: 17.0066\n",
      "Surr1 - mean: 0.1300, std: 16.3758\n",
      "Surr2 - mean: 0.1300, std: 16.3758\n",
      "Policy loss: -0.130023\n",
      "Value loss: 207.996338\n",
      "Entropy bonus: 0.010953\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0991, std: 0.1034\n",
      "Log probs new - mean: -1.0890, std: 0.0756\n",
      "Log prob diff - mean: 0.0100, std: 0.0587\n",
      "Ratios - mean: 1.0118, std: 0.0586\n",
      "Advantages - mean: -1.9772, std: 10.7585\n",
      "Surr1 - mean: -2.0714, std: 10.9473\n",
      "Surr2 - mean: -2.0714, std: 10.9473\n",
      "Policy loss: 2.071403\n",
      "Value loss: 149.965424\n",
      "Entropy bonus: 0.010954\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.1069, std: 0.1052\n",
      "Log probs new - mean: -1.0972, std: 0.0789\n",
      "Log prob diff - mean: 0.0097, std: 0.0594\n",
      "Ratios - mean: 1.0115, std: 0.0593\n",
      "Advantages - mean: -0.3826, std: 23.1459\n",
      "Surr1 - mean: -0.3884, std: 23.3699\n",
      "Surr2 - mean: -0.3884, std: 23.3699\n",
      "Policy loss: 0.388443\n",
      "Value loss: 336.105865\n",
      "Entropy bonus: 0.010954\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0901, std: 0.1117\n",
      "Log probs new - mean: -1.0936, std: 0.0778\n",
      "Log prob diff - mean: -0.0035, std: 0.0639\n",
      "Ratios - mean: 0.9985, std: 0.0635\n",
      "Advantages - mean: 0.1674, std: 18.4299\n",
      "Surr1 - mean: 0.0161, std: 18.0626\n",
      "Surr2 - mean: 0.0161, std: 18.0626\n",
      "Policy loss: -0.016050\n",
      "Value loss: 232.914642\n",
      "Entropy bonus: 0.010954\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0703, std: 0.1167\n",
      "Log probs new - mean: -1.0888, std: 0.0776\n",
      "Log prob diff - mean: -0.0186, std: 0.0651\n",
      "Ratios - mean: 0.9837, std: 0.0646\n",
      "Advantages - mean: -0.8768, std: 21.2421\n",
      "Surr1 - mean: -1.0811, std: 20.4248\n",
      "Surr2 - mean: -1.0811, std: 20.4248\n",
      "Policy loss: 1.081119\n",
      "Value loss: 300.124939\n",
      "Entropy bonus: 0.010954\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0981, std: 0.1123\n",
      "Log probs new - mean: -1.0971, std: 0.0816\n",
      "Log prob diff - mean: 0.0010, std: 0.0674\n",
      "Ratios - mean: 1.0033, std: 0.0673\n",
      "Advantages - mean: 3.0427, std: 28.6256\n",
      "Surr1 - mean: 2.9804, std: 28.0549\n",
      "Surr2 - mean: 2.9804, std: 28.0549\n",
      "Policy loss: -2.980444\n",
      "Value loss: 440.021759\n",
      "Entropy bonus: 0.010954\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0920, std: 0.1072\n",
      "Log probs new - mean: -1.0881, std: 0.0758\n",
      "Log prob diff - mean: 0.0039, std: 0.0723\n",
      "Ratios - mean: 1.0065, std: 0.0721\n",
      "Advantages - mean: -1.0076, std: 21.3722\n",
      "Surr1 - mean: -0.6683, std: 22.4383\n",
      "Surr2 - mean: -0.6683, std: 22.4383\n",
      "Policy loss: 0.668308\n",
      "Value loss: 304.123596\n",
      "Entropy bonus: 0.010954\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0850, std: 0.1139\n",
      "Log probs new - mean: -1.0943, std: 0.0781\n",
      "Log prob diff - mean: -0.0094, std: 0.0749\n",
      "Ratios - mean: 0.9934, std: 0.0746\n",
      "Advantages - mean: 0.0143, std: 18.2059\n",
      "Surr1 - mean: 0.0703, std: 18.2949\n",
      "Surr2 - mean: 0.0703, std: 18.2949\n",
      "Policy loss: -0.070257\n",
      "Value loss: 229.689392\n",
      "Entropy bonus: 0.010954\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.1022, std: 0.1062\n",
      "Log probs new - mean: -1.0901, std: 0.0807\n",
      "Log prob diff - mean: 0.0121, std: 0.0761\n",
      "Ratios - mean: 1.0150, std: 0.0762\n",
      "Advantages - mean: 1.6125, std: 25.2808\n",
      "Surr1 - mean: 1.8057, std: 25.2757\n",
      "Surr2 - mean: 1.8057, std: 25.2757\n",
      "Policy loss: -1.805720\n",
      "Value loss: 364.001068\n",
      "Entropy bonus: 0.010954\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0840, std: 0.1161\n",
      "Log probs new - mean: -1.0973, std: 0.0795\n",
      "Log prob diff - mean: -0.0133, std: 0.0808\n",
      "Ratios - mean: 0.9900, std: 0.0805\n",
      "Advantages - mean: 0.8770, std: 19.6468\n",
      "Surr1 - mean: 0.8660, std: 18.6713\n",
      "Surr2 - mean: 0.8660, std: 18.6713\n",
      "Policy loss: -0.866036\n",
      "Value loss: 246.515671\n",
      "Entropy bonus: 0.010954\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0933, std: 0.1143\n",
      "Log probs new - mean: -1.1009, std: 0.0800\n",
      "Log prob diff - mean: -0.0075, std: 0.0854\n",
      "Ratios - mean: 0.9960, std: 0.0849\n",
      "Advantages - mean: -0.0314, std: 19.6504\n",
      "Surr1 - mean: -0.1038, std: 19.3393\n",
      "Surr2 - mean: -0.1038, std: 19.3393\n",
      "Policy loss: 0.103843\n",
      "Value loss: 256.343353\n",
      "Entropy bonus: 0.010953\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.1163, std: 0.1073\n",
      "Log probs new - mean: -1.1067, std: 0.0848\n",
      "Log prob diff - mean: 0.0096, std: 0.0846\n",
      "Ratios - mean: 1.0132, std: 0.0843\n",
      "Advantages - mean: -3.0819, std: 16.4357\n",
      "Surr1 - mean: -3.3388, std: 17.2946\n",
      "Surr2 - mean: -3.3388, std: 17.2946\n",
      "Policy loss: 3.338785\n",
      "Value loss: 238.907318\n",
      "Entropy bonus: 0.010953\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.1076, std: 0.1092\n",
      "Log probs new - mean: -1.1007, std: 0.0842\n",
      "Log prob diff - mean: 0.0069, std: 0.0910\n",
      "Ratios - mean: 1.0110, std: 0.0908\n",
      "Advantages - mean: -0.6532, std: 21.9189\n",
      "Surr1 - mean: -0.5297, std: 23.0868\n",
      "Surr2 - mean: -0.5297, std: 23.0868\n",
      "Policy loss: 0.529685\n",
      "Value loss: 309.743256\n",
      "Entropy bonus: 0.010952\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.1066, std: 0.1172\n",
      "Log probs new - mean: -1.1150, std: 0.0847\n",
      "Log prob diff - mean: -0.0085, std: 0.0920\n",
      "Ratios - mean: 0.9957, std: 0.0914\n",
      "Advantages - mean: -0.0305, std: 18.1586\n",
      "Surr1 - mean: 0.3862, std: 18.1718\n",
      "Surr2 - mean: 0.3862, std: 18.1718\n",
      "Policy loss: -0.386217\n",
      "Value loss: 227.746689\n",
      "Entropy bonus: 0.010951\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0958, std: 0.1133\n",
      "Log probs new - mean: -1.1020, std: 0.0832\n",
      "Log prob diff - mean: -0.0062, std: 0.1046\n",
      "Ratios - mean: 0.9991, std: 0.1038\n",
      "Advantages - mean: 2.6201, std: 19.8121\n",
      "Surr1 - mean: 2.7032, std: 20.4414\n",
      "Surr2 - mean: 2.7032, std: 20.4414\n",
      "Policy loss: -2.703173\n",
      "Value loss: 231.370483\n",
      "Entropy bonus: 0.010950\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.1001, std: 0.1129\n",
      "Log probs new - mean: -1.1027, std: 0.0873\n",
      "Log prob diff - mean: -0.0026, std: 0.1076\n",
      "Ratios - mean: 1.0030, std: 0.1071\n",
      "Advantages - mean: 0.0613, std: 12.0396\n",
      "Surr1 - mean: 0.4052, std: 11.9503\n",
      "Surr2 - mean: 0.4052, std: 11.9503\n",
      "Policy loss: -0.405247\n",
      "Value loss: 134.826645\n",
      "Entropy bonus: 0.010948\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0719, std: 0.1273\n",
      "Log probs new - mean: -1.1182, std: 0.0765\n",
      "Log prob diff - mean: -0.0464, std: 0.1037\n",
      "Ratios - mean: 0.9596, std: 0.1022\n",
      "Advantages - mean: -4.1706, std: 24.7885\n",
      "Surr1 - mean: -3.1850, std: 24.3179\n",
      "Surr2 - mean: -3.1850, std: 24.3179\n",
      "Policy loss: 3.185007\n",
      "Value loss: 407.336700\n",
      "Entropy bonus: 0.010949\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.1100, std: 0.1070\n",
      "Log probs new - mean: -1.0964, std: 0.0951\n",
      "Log prob diff - mean: 0.0136, std: 0.1197\n",
      "Ratios - mean: 1.0207, std: 0.1192\n",
      "Advantages - mean: -0.0731, std: 16.4855\n",
      "Surr1 - mean: 0.1981, std: 17.7857\n",
      "Surr2 - mean: 0.1981, std: 17.7857\n",
      "Policy loss: -0.198081\n",
      "Value loss: 198.007355\n",
      "Entropy bonus: 0.010944\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.1043, std: 0.1067\n",
      "Log probs new - mean: -1.0915, std: 0.0960\n",
      "Log prob diff - mean: 0.0128, std: 0.1268\n",
      "Ratios - mean: 1.0208, std: 0.1261\n",
      "Advantages - mean: 4.1924, std: 29.2268\n",
      "Surr1 - mean: 4.3467, std: 29.7351\n",
      "Surr2 - mean: 4.3467, std: 29.7351\n",
      "Policy loss: -4.346717\n",
      "Value loss: 445.365387\n",
      "Entropy bonus: 0.010942\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.1016, std: 0.1200\n",
      "Log probs new - mean: -1.1273, std: 0.0865\n",
      "Log prob diff - mean: -0.0257, std: 0.1269\n",
      "Ratios - mean: 0.9824, std: 0.1241\n",
      "Advantages - mean: 1.1524, std: 18.9021\n",
      "Surr1 - mean: 1.1314, std: 19.0099\n",
      "Surr2 - mean: 1.1314, std: 19.0099\n",
      "Policy loss: -1.131408\n",
      "Value loss: 226.435120\n",
      "Entropy bonus: 0.010938\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0874, std: 0.1132\n",
      "Log probs new - mean: -1.1042, std: 0.0965\n",
      "Log prob diff - mean: -0.0168, std: 0.1447\n",
      "Ratios - mean: 0.9935, std: 0.1427\n",
      "Advantages - mean: -1.4066, std: 19.1672\n",
      "Surr1 - mean: -1.2332, std: 19.4333\n",
      "Surr2 - mean: -1.2332, std: 19.4333\n",
      "Policy loss: 1.233217\n",
      "Value loss: 260.020020\n",
      "Entropy bonus: 0.010935\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0903, std: 0.1162\n",
      "Log probs new - mean: -1.1134, std: 0.0959\n",
      "Log prob diff - mean: -0.0231, std: 0.1469\n",
      "Ratios - mean: 0.9875, std: 0.1439\n",
      "Advantages - mean: -4.7014, std: 12.7255\n",
      "Surr1 - mean: -4.8216, std: 12.6850\n",
      "Surr2 - mean: -4.8216, std: 12.6850\n",
      "Policy loss: 4.821614\n",
      "Value loss: 205.181900\n",
      "Entropy bonus: 0.010932\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.1121, std: 0.1063\n",
      "Log probs new - mean: -1.0907, std: 0.1118\n",
      "Log prob diff - mean: 0.0214, std: 0.1482\n",
      "Ratios - mean: 1.0324, std: 0.1472\n",
      "Advantages - mean: 0.9507, std: 20.9182\n",
      "Surr1 - mean: 1.0484, std: 19.8933\n",
      "Surr2 - mean: 1.0480, std: 19.8931\n",
      "Policy loss: -1.047956\n",
      "Value loss: 266.848541\n",
      "Entropy bonus: 0.010927\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0994, std: 0.1151\n",
      "Log probs new - mean: -1.1170, std: 0.1042\n",
      "Log prob diff - mean: -0.0176, std: 0.1556\n",
      "Ratios - mean: 0.9942, std: 0.1523\n",
      "Advantages - mean: -3.6568, std: 12.9536\n",
      "Surr1 - mean: -3.4898, std: 12.9256\n",
      "Surr2 - mean: -3.4906, std: 12.9224\n",
      "Policy loss: 3.491740\n",
      "Value loss: 190.815689\n",
      "Entropy bonus: 0.010923\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.1015, std: 0.1081\n",
      "Log probs new - mean: -1.0958, std: 0.1188\n",
      "Log prob diff - mean: 0.0058, std: 0.1686\n",
      "Ratios - mean: 1.0196, std: 0.1658\n",
      "Advantages - mean: -3.0214, std: 12.6808\n",
      "Surr1 - mean: -3.1341, std: 13.2579\n",
      "Surr2 - mean: -3.1402, std: 13.2544\n",
      "Policy loss: 3.143937\n",
      "Value loss: 177.454636\n",
      "Entropy bonus: 0.010916\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0949, std: 0.1102\n",
      "Log probs new - mean: -1.1009, std: 0.1224\n",
      "Log prob diff - mean: -0.0060, std: 0.1795\n",
      "Ratios - mean: 1.0096, std: 0.1757\n",
      "Advantages - mean: 1.2685, std: 22.4924\n",
      "Surr1 - mean: 1.0619, std: 24.3273\n",
      "Surr2 - mean: 1.0382, std: 24.2066\n",
      "Policy loss: -1.011539\n",
      "Value loss: 296.343445\n",
      "Entropy bonus: 0.010911\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0815, std: 0.1222\n",
      "Log probs new - mean: -1.1507, std: 0.1013\n",
      "Log prob diff - mean: -0.0692, std: 0.1719\n",
      "Ratios - mean: 0.9470, std: 0.1651\n",
      "Advantages - mean: 5.5507, std: 25.4821\n",
      "Surr1 - mean: 5.5254, std: 25.2544\n",
      "Surr2 - mean: 5.5192, std: 25.1624\n",
      "Policy loss: -5.462635\n",
      "Value loss: 333.982605\n",
      "Entropy bonus: 0.010903\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0967, std: 0.1106\n",
      "Log probs new - mean: -1.1039, std: 0.1289\n",
      "Log prob diff - mean: -0.0072, std: 0.1862\n",
      "Ratios - mean: 1.0096, std: 0.1824\n",
      "Advantages - mean: 0.7353, std: 21.4901\n",
      "Surr1 - mean: 1.8651, std: 23.7281\n",
      "Surr2 - mean: 1.7680, std: 23.4401\n",
      "Policy loss: -1.734291\n",
      "Value loss: 279.011780\n",
      "Entropy bonus: 0.010902\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.1140, std: 0.1064\n",
      "Log probs new - mean: -1.0955, std: 0.1310\n",
      "Log prob diff - mean: 0.0185, std: 0.1785\n",
      "Ratios - mean: 1.0342, std: 0.1750\n",
      "Advantages - mean: -0.9549, std: 12.1273\n",
      "Surr1 - mean: -0.9985, std: 12.1766\n",
      "Surr2 - mean: -1.0038, std: 12.1742\n",
      "Policy loss: 1.035741\n",
      "Value loss: 141.650833\n",
      "Entropy bonus: 0.010901\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0982, std: 0.1178\n",
      "Log probs new - mean: -1.1315, std: 0.1196\n",
      "Log prob diff - mean: -0.0334, std: 0.1852\n",
      "Ratios - mean: 0.9835, std: 0.1788\n",
      "Advantages - mean: -2.0544, std: 17.4169\n",
      "Surr1 - mean: -1.9125, std: 17.9788\n",
      "Surr2 - mean: -1.9454, std: 17.9856\n",
      "Policy loss: 1.974800\n",
      "Value loss: 231.765198\n",
      "Entropy bonus: 0.010899\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.1084, std: 0.1182\n",
      "Log probs new - mean: -1.1358, std: 0.1157\n",
      "Log prob diff - mean: -0.0274, std: 0.1793\n",
      "Ratios - mean: 0.9882, std: 0.1724\n",
      "Advantages - mean: 1.9805, std: 23.3610\n",
      "Surr1 - mean: 1.2908, std: 21.1497\n",
      "Surr2 - mean: 1.3661, std: 21.4391\n",
      "Policy loss: -1.254537\n",
      "Value loss: 307.373901\n",
      "Entropy bonus: 0.010897\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.1089, std: 0.1062\n",
      "Log probs new - mean: -1.0932, std: 0.1342\n",
      "Log prob diff - mean: 0.0157, std: 0.1877\n",
      "Ratios - mean: 1.0329, std: 0.1833\n",
      "Advantages - mean: 0.8510, std: 19.3483\n",
      "Surr1 - mean: 1.2765, std: 22.0482\n",
      "Surr2 - mean: 1.2099, std: 21.9038\n",
      "Policy loss: -1.176318\n",
      "Value loss: 233.055313\n",
      "Entropy bonus: 0.010898\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.1075, std: 0.1093\n",
      "Log probs new - mean: -1.0993, std: 0.1335\n",
      "Log prob diff - mean: 0.0082, std: 0.1903\n",
      "Ratios - mean: 1.0257, std: 0.1854\n",
      "Advantages - mean: -2.5569, std: 13.6721\n",
      "Surr1 - mean: -2.5152, std: 15.0695\n",
      "Surr2 - mean: -2.5619, std: 14.9243\n",
      "Policy loss: 2.601940\n",
      "Value loss: 178.997803\n",
      "Entropy bonus: 0.010897\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.1045, std: 0.1071\n",
      "Log probs new - mean: -1.0922, std: 0.1345\n",
      "Log prob diff - mean: 0.0124, std: 0.1910\n",
      "Ratios - mean: 1.0301, std: 0.1864\n",
      "Advantages - mean: -2.7615, std: 13.0090\n",
      "Surr1 - mean: -2.4966, std: 13.6339\n",
      "Surr2 - mean: -2.5466, std: 13.6186\n",
      "Policy loss: 2.566343\n",
      "Value loss: 172.763733\n",
      "Entropy bonus: 0.010899\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0660, std: 0.1157\n",
      "Log probs new - mean: -1.1376, std: 0.1188\n",
      "Log prob diff - mean: -0.0716, std: 0.1967\n",
      "Ratios - mean: 0.9490, std: 0.1894\n",
      "Advantages - mean: -0.8400, std: 15.7664\n",
      "Surr1 - mean: -0.9574, std: 14.8004\n",
      "Surr2 - mean: -0.9526, std: 14.9163\n",
      "Policy loss: 1.024873\n",
      "Value loss: 187.617111\n",
      "Entropy bonus: 0.010900\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.1062, std: 0.1076\n",
      "Log probs new - mean: -1.0968, std: 0.1299\n",
      "Log prob diff - mean: 0.0094, std: 0.1879\n",
      "Ratios - mean: 1.0265, std: 0.1826\n",
      "Advantages - mean: -4.1031, std: 14.7465\n",
      "Surr1 - mean: -4.0889, std: 13.8473\n",
      "Surr2 - mean: -4.1244, std: 13.9870\n",
      "Policy loss: 4.175637\n",
      "Value loss: 214.261566\n",
      "Entropy bonus: 0.010904\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0981, std: 0.1118\n",
      "Log probs new - mean: -1.1083, std: 0.1276\n",
      "Log prob diff - mean: -0.0102, std: 0.1921\n",
      "Ratios - mean: 1.0075, std: 0.1861\n",
      "Advantages - mean: 1.6121, std: 18.3719\n",
      "Surr1 - mean: 1.7793, std: 17.8443\n",
      "Surr2 - mean: 1.7524, std: 17.9733\n",
      "Policy loss: -1.704010\n",
      "Value loss: 205.300980\n",
      "Entropy bonus: 0.010903\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0853, std: 0.1140\n",
      "Log probs new - mean: -1.1183, std: 0.1215\n",
      "Log prob diff - mean: -0.0330, std: 0.1940\n",
      "Ratios - mean: 0.9854, std: 0.1872\n",
      "Advantages - mean: -1.2655, std: 18.0832\n",
      "Surr1 - mean: -0.6205, std: 19.1428\n",
      "Surr2 - mean: -0.6855, std: 19.1314\n",
      "Policy loss: 0.704577\n",
      "Value loss: 229.581284\n",
      "Entropy bonus: 0.010909\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.1005, std: 0.1068\n",
      "Log probs new - mean: -1.0935, std: 0.1253\n",
      "Log prob diff - mean: 0.0070, std: 0.1878\n",
      "Ratios - mean: 1.0240, std: 0.1820\n",
      "Advantages - mean: 0.3988, std: 21.7974\n",
      "Surr1 - mean: -0.3472, std: 20.9098\n",
      "Surr2 - mean: -0.3185, std: 21.0908\n",
      "Policy loss: 0.396869\n",
      "Value loss: 284.090515\n",
      "Entropy bonus: 0.010911\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0830, std: 0.1066\n",
      "Log probs new - mean: -1.0943, std: 0.1248\n",
      "Log prob diff - mean: -0.0112, std: 0.1950\n",
      "Ratios - mean: 1.0071, std: 0.1890\n",
      "Advantages - mean: 4.2891, std: 25.4753\n",
      "Surr1 - mean: 4.1490, std: 24.6094\n",
      "Surr2 - mean: 4.1829, std: 24.8101\n",
      "Policy loss: -4.087021\n",
      "Value loss: 338.044434\n",
      "Entropy bonus: 0.010914\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0836, std: 0.1062\n",
      "Log probs new - mean: -1.0933, std: 0.1194\n",
      "Log prob diff - mean: -0.0097, std: 0.1904\n",
      "Ratios - mean: 1.0077, std: 0.1846\n",
      "Advantages - mean: -2.4868, std: 19.4220\n",
      "Surr1 - mean: -2.1101, std: 21.0887\n",
      "Surr2 - mean: -2.1445, std: 21.1214\n",
      "Policy loss: 2.158699\n",
      "Value loss: 268.000854\n",
      "Entropy bonus: 0.010920\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0856, std: 0.1117\n",
      "Log probs new - mean: -1.1087, std: 0.1104\n",
      "Log prob diff - mean: -0.0232, std: 0.1817\n",
      "Ratios - mean: 0.9928, std: 0.1753\n",
      "Advantages - mean: 3.5960, std: 27.8410\n",
      "Surr1 - mean: 2.7724, std: 24.7928\n",
      "Surr2 - mean: 2.7956, std: 24.9685\n",
      "Policy loss: -2.739288\n",
      "Value loss: 404.050293\n",
      "Entropy bonus: 0.010924\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0864, std: 0.1151\n",
      "Log probs new - mean: -1.1173, std: 0.1011\n",
      "Log prob diff - mean: -0.0309, std: 0.1741\n",
      "Ratios - mean: 0.9839, std: 0.1676\n",
      "Advantages - mean: -0.3145, std: 20.8727\n",
      "Surr1 - mean: -0.0635, std: 21.4528\n",
      "Surr2 - mean: -0.0802, std: 21.4738\n",
      "Policy loss: 0.083651\n",
      "Value loss: 270.700287\n",
      "Entropy bonus: 0.010930\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.1090, std: 0.1099\n",
      "Log probs new - mean: -1.1014, std: 0.1000\n",
      "Log prob diff - mean: 0.0075, std: 0.1607\n",
      "Ratios - mean: 1.0200, std: 0.1552\n",
      "Advantages - mean: -0.9049, std: 17.8507\n",
      "Surr1 - mean: -1.0902, std: 17.4033\n",
      "Surr2 - mean: -1.0965, std: 17.4105\n",
      "Policy loss: 1.096541\n",
      "Value loss: 218.681366\n",
      "Entropy bonus: 0.010935\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0746, std: 0.1175\n",
      "Log probs new - mean: -1.1248, std: 0.0856\n",
      "Log prob diff - mean: -0.0502, std: 0.1640\n",
      "Ratios - mean: 0.9637, std: 0.1575\n",
      "Advantages - mean: 1.7450, std: 21.3884\n",
      "Surr1 - mean: 1.1391, std: 18.5784\n",
      "Surr2 - mean: 1.1429, std: 18.5974\n",
      "Policy loss: -1.138631\n",
      "Value loss: 260.810669\n",
      "Entropy bonus: 0.010940\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.1118, std: 0.1084\n",
      "Log probs new - mean: -1.0981, std: 0.0935\n",
      "Log prob diff - mean: 0.0137, std: 0.1509\n",
      "Ratios - mean: 1.0248, std: 0.1459\n",
      "Advantages - mean: 4.1327, std: 23.4173\n",
      "Surr1 - mean: 4.3483, std: 22.4181\n",
      "Surr2 - mean: 4.3483, std: 22.4181\n",
      "Policy loss: -4.348286\n",
      "Value loss: 288.222107\n",
      "Entropy bonus: 0.010944\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0942, std: 0.1059\n",
      "Log probs new - mean: -1.0899, std: 0.0896\n",
      "Log prob diff - mean: 0.0043, std: 0.1545\n",
      "Ratios - mean: 1.0158, std: 0.1495\n",
      "Advantages - mean: -2.4262, std: 17.9258\n",
      "Surr1 - mean: -2.5820, std: 16.4157\n",
      "Surr2 - mean: -2.5820, std: 16.4157\n",
      "Policy loss: 2.582020\n",
      "Value loss: 237.198074\n",
      "Entropy bonus: 0.010948\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0869, std: 0.1089\n",
      "Log probs new - mean: -1.0990, std: 0.0842\n",
      "Log prob diff - mean: -0.0122, std: 0.1539\n",
      "Ratios - mean: 0.9992, std: 0.1486\n",
      "Advantages - mean: 0.2387, std: 19.7148\n",
      "Surr1 - mean: 0.6777, std: 20.0839\n",
      "Surr2 - mean: 0.6777, std: 20.0839\n",
      "Policy loss: -0.677733\n",
      "Value loss: 239.979843\n",
      "Entropy bonus: 0.010952\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0726, std: 0.1119\n",
      "Log probs new - mean: -1.1069, std: 0.0749\n",
      "Log prob diff - mean: -0.0343, std: 0.1531\n",
      "Ratios - mean: 0.9774, std: 0.1477\n",
      "Advantages - mean: 0.6230, std: 25.2077\n",
      "Surr1 - mean: 0.3305, std: 25.5543\n",
      "Surr2 - mean: 0.3305, std: 25.5543\n",
      "Policy loss: -0.330470\n",
      "Value loss: 357.665710\n",
      "Entropy bonus: 0.010955\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.1049, std: 0.1137\n",
      "Log probs new - mean: -1.1009, std: 0.0779\n",
      "Log prob diff - mean: 0.0040, std: 0.1467\n",
      "Ratios - mean: 1.0138, std: 0.1414\n",
      "Advantages - mean: 0.8995, std: 23.9522\n",
      "Surr1 - mean: 1.3868, std: 26.5924\n",
      "Surr2 - mean: 1.3868, std: 26.5924\n",
      "Policy loss: -1.386775\n",
      "Value loss: 310.214203\n",
      "Entropy bonus: 0.010957\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.1141, std: 0.1125\n",
      "Log probs new - mean: -1.1072, std: 0.0724\n",
      "Log prob diff - mean: 0.0070, std: 0.1339\n",
      "Ratios - mean: 1.0156, std: 0.1291\n",
      "Advantages - mean: 2.2797, std: 18.0580\n",
      "Surr1 - mean: 2.0348, std: 18.3178\n",
      "Surr2 - mean: 2.0348, std: 18.3178\n",
      "Policy loss: -2.034766\n",
      "Value loss: 190.495361\n",
      "Entropy bonus: 0.010958\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0924, std: 0.1089\n",
      "Log probs new - mean: -1.0936, std: 0.0747\n",
      "Log prob diff - mean: -0.0012, std: 0.1417\n",
      "Ratios - mean: 1.0085, std: 0.1370\n",
      "Advantages - mean: 2.8822, std: 16.9442\n",
      "Surr1 - mean: 3.1673, std: 17.7570\n",
      "Surr2 - mean: 3.1673, std: 17.7570\n",
      "Policy loss: -3.167321\n",
      "Value loss: 166.607391\n",
      "Entropy bonus: 0.010959\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0976, std: 0.1142\n",
      "Log probs new - mean: -1.1064, std: 0.0702\n",
      "Log prob diff - mean: -0.0088, std: 0.1370\n",
      "Ratios - mean: 1.0002, std: 0.1323\n",
      "Advantages - mean: 4.1054, std: 18.8914\n",
      "Surr1 - mean: 3.8720, std: 17.7077\n",
      "Surr2 - mean: 3.8720, std: 17.7077\n",
      "Policy loss: -3.871968\n",
      "Value loss: 192.810089\n",
      "Entropy bonus: 0.010959\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0856, std: 0.1172\n",
      "Log probs new - mean: -1.1122, std: 0.0640\n",
      "Log prob diff - mean: -0.0266, std: 0.1394\n",
      "Ratios - mean: 0.9830, std: 0.1343\n",
      "Advantages - mean: 1.5181, std: 19.9841\n",
      "Surr1 - mean: 1.8899, std: 20.7192\n",
      "Surr2 - mean: 1.8899, std: 20.7192\n",
      "Policy loss: -1.889948\n",
      "Value loss: 231.923798\n",
      "Entropy bonus: 0.010961\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.1060, std: 0.1118\n",
      "Log probs new - mean: -1.1005, std: 0.0677\n",
      "Log prob diff - mean: 0.0055, std: 0.1342\n",
      "Ratios - mean: 1.0142, std: 0.1295\n",
      "Advantages - mean: -0.5582, std: 19.6466\n",
      "Surr1 - mean: -0.1492, std: 21.0065\n",
      "Surr2 - mean: -0.1492, std: 21.0065\n",
      "Policy loss: 0.149206\n",
      "Value loss: 244.788071\n",
      "Entropy bonus: 0.010962\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0698, std: 0.1069\n",
      "Log probs new - mean: -1.0966, std: 0.0683\n",
      "Log prob diff - mean: -0.0268, std: 0.1451\n",
      "Ratios - mean: 0.9836, std: 0.1402\n",
      "Advantages - mean: -2.3397, std: 14.9906\n",
      "Surr1 - mean: -2.0274, std: 15.1913\n",
      "Surr2 - mean: -2.0274, std: 15.1913\n",
      "Policy loss: 2.027445\n",
      "Value loss: 185.418457\n",
      "Entropy bonus: 0.010963\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0912, std: 0.1091\n",
      "Log probs new - mean: -1.0975, std: 0.0675\n",
      "Log prob diff - mean: -0.0063, std: 0.1393\n",
      "Ratios - mean: 1.0030, std: 0.1343\n",
      "Advantages - mean: -0.2066, std: 20.9880\n",
      "Surr1 - mean: -0.1781, std: 19.8527\n",
      "Surr2 - mean: -0.1781, std: 19.8527\n",
      "Policy loss: 0.178072\n",
      "Value loss: 267.507629\n",
      "Entropy bonus: 0.010964\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0817, std: 0.1171\n",
      "Log probs new - mean: -1.1125, std: 0.0603\n",
      "Log prob diff - mean: -0.0308, std: 0.1443\n",
      "Ratios - mean: 0.9796, std: 0.1387\n",
      "Advantages - mean: -2.0847, std: 25.5102\n",
      "Surr1 - mean: -2.1627, std: 25.0829\n",
      "Surr2 - mean: -2.1627, std: 25.0829\n",
      "Policy loss: 2.162664\n",
      "Value loss: 391.660889\n",
      "Entropy bonus: 0.010964\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0936, std: 0.1181\n",
      "Log probs new - mean: -1.1131, std: 0.0597\n",
      "Log prob diff - mean: -0.0195, std: 0.1429\n",
      "Ratios - mean: 0.9904, std: 0.1370\n",
      "Advantages - mean: 0.6506, std: 26.2462\n",
      "Surr1 - mean: 0.6152, std: 24.7510\n",
      "Surr2 - mean: 0.6152, std: 24.7510\n",
      "Policy loss: -0.615243\n",
      "Value loss: 381.260864\n",
      "Entropy bonus: 0.010965\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.1120, std: 0.1018\n",
      "Log probs new - mean: -1.0867, std: 0.0660\n",
      "Log prob diff - mean: 0.0252, std: 0.1307\n",
      "Ratios - mean: 1.0338, std: 0.1257\n",
      "Advantages - mean: 2.3543, std: 27.3134\n",
      "Surr1 - mean: 1.6806, std: 26.0463\n",
      "Surr2 - mean: 1.6806, std: 26.0463\n",
      "Policy loss: -1.680623\n",
      "Value loss: 394.966980\n",
      "Entropy bonus: 0.010966\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0946, std: 0.1122\n",
      "Log probs new - mean: -1.1022, std: 0.0621\n",
      "Log prob diff - mean: -0.0076, std: 0.1441\n",
      "Ratios - mean: 1.0024, std: 0.1384\n",
      "Advantages - mean: -3.2347, std: 19.3485\n",
      "Surr1 - mean: -3.0374, std: 19.8000\n",
      "Surr2 - mean: -3.0374, std: 19.8000\n",
      "Policy loss: 3.037372\n",
      "Value loss: 269.099701\n",
      "Entropy bonus: 0.010967\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0886, std: 0.1146\n",
      "Log probs new - mean: -1.1058, std: 0.0592\n",
      "Log prob diff - mean: -0.0173, std: 0.1458\n",
      "Ratios - mean: 0.9930, std: 0.1400\n",
      "Advantages - mean: 0.4153, std: 21.4898\n",
      "Surr1 - mean: 0.1482, std: 20.5711\n",
      "Surr2 - mean: 0.1482, std: 20.5711\n",
      "Policy loss: -0.148201\n",
      "Value loss: 271.002014\n",
      "Entropy bonus: 0.010968\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.1032, std: 0.1152\n",
      "Log probs new - mean: -1.1055, std: 0.0557\n",
      "Log prob diff - mean: -0.0023, std: 0.1414\n",
      "Ratios - mean: 1.0073, std: 0.1359\n",
      "Advantages - mean: -1.7335, std: 20.3615\n",
      "Surr1 - mean: -1.3202, std: 22.0868\n",
      "Surr2 - mean: -1.3202, std: 22.0868\n",
      "Policy loss: 1.320231\n",
      "Value loss: 269.757965\n",
      "Entropy bonus: 0.010969\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0904, std: 0.1118\n",
      "Log probs new - mean: -1.1019, std: 0.0585\n",
      "Log prob diff - mean: -0.0115, std: 0.1444\n",
      "Ratios - mean: 0.9985, std: 0.1387\n",
      "Advantages - mean: -0.2820, std: 22.5894\n",
      "Surr1 - mean: -0.2883, std: 23.9507\n",
      "Surr2 - mean: -0.2883, std: 23.9507\n",
      "Policy loss: 0.288328\n",
      "Value loss: 301.198242\n",
      "Entropy bonus: 0.010969\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0849, std: 0.1137\n",
      "Log probs new - mean: -1.1074, std: 0.0570\n",
      "Log prob diff - mean: -0.0226, std: 0.1466\n",
      "Ratios - mean: 0.9879, std: 0.1406\n",
      "Advantages - mean: -1.4730, std: 12.7695\n",
      "Surr1 - mean: -1.2930, std: 12.4226\n",
      "Surr2 - mean: -1.2930, std: 12.4226\n",
      "Policy loss: 1.292983\n",
      "Value loss: 142.513824\n",
      "Entropy bonus: 0.010970\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.1142, std: 0.1126\n",
      "Log probs new - mean: -1.1025, std: 0.0535\n",
      "Log prob diff - mean: 0.0117, std: 0.1394\n",
      "Ratios - mean: 1.0211, std: 0.1339\n",
      "Advantages - mean: 2.7353, std: 21.8920\n",
      "Surr1 - mean: 2.5336, std: 21.5647\n",
      "Surr2 - mean: 2.5336, std: 21.5647\n",
      "Policy loss: -2.533627\n",
      "Value loss: 260.074615\n",
      "Entropy bonus: 0.010970\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0861, std: 0.1154\n",
      "Log probs new - mean: -1.1094, std: 0.0552\n",
      "Log prob diff - mean: -0.0232, std: 0.1505\n",
      "Ratios - mean: 0.9878, std: 0.1442\n",
      "Advantages - mean: -3.7747, std: 13.1717\n",
      "Surr1 - mean: -3.3836, std: 11.9632\n",
      "Surr2 - mean: -3.3836, std: 11.9632\n",
      "Policy loss: 3.383649\n",
      "Value loss: 175.498398\n",
      "Entropy bonus: 0.010971\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.1239, std: 0.1091\n",
      "Log probs new - mean: -1.0983, std: 0.0534\n",
      "Log prob diff - mean: 0.0256, std: 0.1375\n",
      "Ratios - mean: 1.0350, std: 0.1318\n",
      "Advantages - mean: -4.9698, std: 14.3574\n",
      "Surr1 - mean: -4.8522, std: 14.7669\n",
      "Surr2 - mean: -4.8522, std: 14.7669\n",
      "Policy loss: 4.852185\n",
      "Value loss: 208.145432\n",
      "Entropy bonus: 0.010971\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0714, std: 0.1085\n",
      "Log probs new - mean: -1.1061, std: 0.0590\n",
      "Log prob diff - mean: -0.0346, std: 0.1528\n",
      "Ratios - mean: 0.9770, std: 0.1461\n",
      "Advantages - mean: -1.6678, std: 18.1907\n",
      "Surr1 - mean: -1.6137, std: 16.8732\n",
      "Surr2 - mean: -1.6137, std: 16.8732\n",
      "Policy loss: 1.613740\n",
      "Value loss: 226.518417\n",
      "Entropy bonus: 0.010971\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.1098, std: 0.1034\n",
      "Log probs new - mean: -1.0906, std: 0.0571\n",
      "Log prob diff - mean: 0.0192, std: 0.1417\n",
      "Ratios - mean: 1.0290, std: 0.1357\n",
      "Advantages - mean: -2.5103, std: 22.0638\n",
      "Surr1 - mean: -2.6008, std: 21.9305\n",
      "Surr2 - mean: -2.6008, std: 21.9305\n",
      "Policy loss: 2.600818\n",
      "Value loss: 313.098267\n",
      "Entropy bonus: 0.010971\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0810, std: 0.1138\n",
      "Log probs new - mean: -1.1083, std: 0.0564\n",
      "Log prob diff - mean: -0.0274, std: 0.1573\n",
      "Ratios - mean: 0.9847, std: 0.1510\n",
      "Advantages - mean: -0.5555, std: 19.2545\n",
      "Surr1 - mean: -1.0654, std: 18.2345\n",
      "Surr2 - mean: -1.0654, std: 18.2345\n",
      "Policy loss: 1.065355\n",
      "Value loss: 233.837296\n",
      "Entropy bonus: 0.010971\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0917, std: 0.1071\n",
      "Log probs new - mean: -1.0983, std: 0.0582\n",
      "Log prob diff - mean: -0.0067, std: 0.1516\n",
      "Ratios - mean: 1.0043, std: 0.1451\n",
      "Advantages - mean: -2.7339, std: 14.7102\n",
      "Surr1 - mean: -2.7218, std: 15.3809\n",
      "Surr2 - mean: -2.7233, std: 15.3832\n",
      "Policy loss: 2.723298\n",
      "Value loss: 182.144592\n",
      "Entropy bonus: 0.010971\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.1084, std: 0.1062\n",
      "Log probs new - mean: -1.0937, std: 0.0546\n",
      "Log prob diff - mean: 0.0148, std: 0.1458\n",
      "Ratios - mean: 1.0251, std: 0.1399\n",
      "Advantages - mean: 0.0923, std: 19.5252\n",
      "Surr1 - mean: 0.4052, std: 19.6606\n",
      "Surr2 - mean: 0.4052, std: 19.6606\n",
      "Policy loss: -0.405173\n",
      "Value loss: 232.290375\n",
      "Entropy bonus: 0.010972\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0993, std: 0.1093\n",
      "Log probs new - mean: -1.1005, std: 0.0572\n",
      "Log prob diff - mean: -0.0012, std: 0.1535\n",
      "Ratios - mean: 1.0101, std: 0.1470\n",
      "Advantages - mean: 0.4336, std: 24.3715\n",
      "Surr1 - mean: 0.2600, std: 22.7907\n",
      "Surr2 - mean: 0.2588, std: 22.7915\n",
      "Policy loss: -0.258850\n",
      "Value loss: 333.864105\n",
      "Entropy bonus: 0.010971\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0987, std: 0.1053\n",
      "Log probs new - mean: -1.0954, std: 0.0558\n",
      "Log prob diff - mean: 0.0033, std: 0.1482\n",
      "Ratios - mean: 1.0138, std: 0.1421\n",
      "Advantages - mean: 3.4792, std: 19.2500\n",
      "Surr1 - mean: 3.4502, std: 20.3662\n",
      "Surr2 - mean: 3.4494, std: 20.3672\n",
      "Policy loss: -3.449414\n",
      "Value loss: 200.621490\n",
      "Entropy bonus: 0.010972\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0904, std: 0.1152\n",
      "Log probs new - mean: -1.1074, std: 0.0552\n",
      "Log prob diff - mean: -0.0170, std: 0.1604\n",
      "Ratios - mean: 0.9954, std: 0.1544\n",
      "Advantages - mean: 3.2774, std: 23.9811\n",
      "Surr1 - mean: 3.0817, std: 24.4119\n",
      "Surr2 - mean: 3.0809, std: 24.4126\n",
      "Policy loss: -3.080915\n",
      "Value loss: 302.486359\n",
      "Entropy bonus: 0.010971\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0872, std: 0.1086\n",
      "Log probs new - mean: -1.1043, std: 0.0586\n",
      "Log prob diff - mean: -0.0171, std: 0.1576\n",
      "Ratios - mean: 0.9949, std: 0.1510\n",
      "Advantages - mean: 0.3429, std: 20.1527\n",
      "Surr1 - mean: 0.4851, std: 20.6126\n",
      "Surr2 - mean: 0.4825, std: 20.6160\n",
      "Policy loss: -0.482467\n",
      "Value loss: 241.751740\n",
      "Entropy bonus: 0.010971\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0961, std: 0.1174\n",
      "Log probs new - mean: -1.1081, std: 0.0543\n",
      "Log prob diff - mean: -0.0120, std: 0.1628\n",
      "Ratios - mean: 1.0007, std: 0.1566\n",
      "Advantages - mean: 0.3621, std: 21.3897\n",
      "Surr1 - mean: 0.9490, std: 23.4103\n",
      "Surr2 - mean: 0.9466, std: 23.4133\n",
      "Policy loss: -0.946458\n",
      "Value loss: 266.611786\n",
      "Entropy bonus: 0.010972\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.1059, std: 0.1136\n",
      "Log probs new - mean: -1.1007, std: 0.0530\n",
      "Log prob diff - mean: 0.0052, std: 0.1585\n",
      "Ratios - mean: 1.0173, std: 0.1532\n",
      "Advantages - mean: 0.9155, std: 17.0153\n",
      "Surr1 - mean: 0.5305, std: 17.5947\n",
      "Surr2 - mean: 0.5305, std: 17.5947\n",
      "Policy loss: -0.530493\n",
      "Value loss: 178.974609\n",
      "Entropy bonus: 0.010972\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.1063, std: 0.1080\n",
      "Log probs new - mean: -1.0966, std: 0.0551\n",
      "Log prob diff - mean: 0.0098, std: 0.1546\n",
      "Ratios - mean: 1.0213, std: 0.1489\n",
      "Advantages - mean: -3.3752, std: 13.5671\n",
      "Surr1 - mean: -3.2735, std: 13.5373\n",
      "Surr2 - mean: -3.2873, std: 13.5624\n",
      "Policy loss: 3.287276\n",
      "Value loss: 172.746399\n",
      "Entropy bonus: 0.010971\n",
      "⚠️  WARNING: Policy loss is very small\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.1057, std: 0.1078\n",
      "Log probs new - mean: -1.0970, std: 0.0541\n",
      "Log prob diff - mean: 0.0086, std: 0.1541\n",
      "Ratios - mean: 1.0201, std: 0.1484\n",
      "Advantages - mean: 3.0857, std: 13.6205\n",
      "Surr1 - mean: 3.1324, std: 14.0339\n",
      "Surr2 - mean: 3.1311, std: 14.0354\n",
      "Policy loss: -3.131090\n",
      "Value loss: 111.352325\n",
      "Entropy bonus: 0.010972\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.0784, std: 0.1142\n",
      "Log probs new - mean: -1.1147, std: 0.0593\n",
      "Log prob diff - mean: -0.0363, std: 0.1683\n",
      "Ratios - mean: 0.9778, std: 0.1619\n",
      "Advantages - mean: 1.8217, std: 23.7965\n",
      "Surr1 - mean: 2.1913, std: 23.2173\n",
      "Surr2 - mean: 2.1818, std: 23.2264\n",
      "Policy loss: -2.181801\n",
      "Value loss: 307.551819\n",
      "Entropy bonus: 0.010971\n",
      "\n",
      "=== Policy Loss Debug (Step 0) ===\n",
      "Log probs old - mean: -1.1263, std: 0.0978\n",
      "Log probs new - mean: -1.0828, std: 0.0486\n",
      "Log prob diff - mean: 0.0435, std: 0.1396\n",
      "Ratios - mean: 1.0535, std: 0.1357\n",
      "Advantages - mean: 3.1928, std: 20.1926\n",
      "Surr1 - mean: 3.2432, std: 22.3601\n",
      "Surr2 - mean: 3.2432, std: 22.3601\n",
      "Policy loss: -3.243202\n",
      "Value loss: 209.624161\n",
      "Entropy bonus: 0.010971\n",
      "Iteration 0 | Episodes: 29 | Mean Return: -50.0828 | Std Return: 69.1822 | Mean steps: 71.1034 | Std steps: 29.4412 | Mean losses: total: 259.475813, policy: 0.000788, value: 259.485968, entropy: 0.010943\n",
      "Original advantages - mean: -10.7883, std: 15.6362\n",
      "Normalized advantages - mean: 0.0000, std: 15.6362\n",
      "Iteration 1 | Episodes: 23 | Mean Return: -68.9602 | Std Return: 54.0931 | Mean steps: 92.9565 | Std steps: 28.6986 | Mean losses: total: 173.148096, policy: -0.039061, value: 173.198003, entropy: 0.010845\n",
      "Original advantages - mean: -5.3054, std: 19.7824\n",
      "Normalized advantages - mean: 0.0000, std: 19.7824\n",
      "Iteration 2 | Episodes: 22 | Mean Return: -32.7674 | Std Return: 89.7736 | Mean steps: 100.6818 | Std steps: 59.5806 | Mean losses: total: 207.865459, policy: -0.044041, value: 207.920084, entropy: 0.010583\n",
      "Original advantages - mean: -2.4594, std: 19.2194\n",
      "Normalized advantages - mean: 0.0000, std: 19.2194\n",
      "Iteration 3 | Episodes: 15 | Mean Return: -30.1144 | Std Return: 88.8572 | Mean steps: 138.0000 | Std steps: 61.8471 | Mean losses: total: 186.782029, policy: -0.017252, value: 186.809561, entropy: 0.010280\n",
      "Original advantages - mean: -6.3346, std: 10.8738\n",
      "Normalized advantages - mean: -0.0000, std: 10.8738\n",
      "Iteration 4 | Episodes: 12 | Mean Return: -86.4968 | Std Return: 55.7292 | Mean steps: 187.3333 | Std steps: 45.0265 | Mean losses: total: 77.418687, policy: 0.035236, value: 77.393562, entropy: 0.010111\n",
      "Original advantages - mean: -1.8364, std: 17.8952\n",
      "Normalized advantages - mean: 0.0000, std: 17.8952\n",
      "Iteration 5 | Episodes: 13 | Mean Return: -31.8019 | Std Return: 92.2598 | Mean steps: 158.3846 | Std steps: 57.1173 | Mean losses: total: 162.912946, policy: -0.079340, value: 163.002663, entropy: 0.010376\n",
      "Original advantages - mean: -5.2241, std: 15.2215\n",
      "Normalized advantages - mean: 0.0000, std: 15.2215\n",
      "Iteration 6 | Episodes: 14 | Mean Return: -64.4593 | Std Return: 74.9725 | Mean steps: 151.0000 | Std steps: 45.1442 | Mean losses: total: 126.660021, policy: -0.075023, value: 126.745423, entropy: 0.010379\n",
      "Original advantages - mean: -5.6165, std: 10.8261\n",
      "Normalized advantages - mean: -0.0000, std: 10.8261\n",
      "Iteration 7 | Episodes: 11 | Mean Return: -87.4966 | Std Return: 48.5952 | Mean steps: 198.5455 | Std steps: 7.4512 | Mean losses: total: 72.566659, policy: 0.021323, value: 72.555469, entropy: 0.010132\n",
      "Original advantages - mean: -5.1133, std: 12.4275\n",
      "Normalized advantages - mean: -0.0000, std: 12.4275\n",
      "Iteration 8 | Episodes: 12 | Mean Return: -68.3753 | Std Return: 71.5304 | Mean steps: 174.2500 | Std steps: 61.2687 | Mean losses: total: 89.129681, policy: -0.014192, value: 89.153971, entropy: 0.010098\n",
      "Original advantages - mean: -1.0554, std: 17.7282\n",
      "Normalized advantages - mean: -0.0000, std: 17.7282\n",
      "Iteration 9 | Episodes: 13 | Mean Return: -30.2002 | Std Return: 91.4937 | Mean steps: 157.8462 | Std steps: 56.8586 | Mean losses: total: 160.987820, policy: -0.037179, value: 161.035041, entropy: 0.010042\n",
      "Original advantages - mean: -3.1575, std: 16.8196\n",
      "Normalized advantages - mean: 0.0000, std: 16.8196\n",
      "Iteration 10 | Episodes: 14 | Mean Return: -39.0736 | Std Return: 95.3278 | Mean steps: 151.0714 | Std steps: 74.8021 | Mean losses: total: 149.856121, policy: -0.195188, value: 150.061161, entropy: 0.009852\n",
      "Original advantages - mean: -5.2573, std: 11.1185\n",
      "Normalized advantages - mean: 0.0000, std: 11.1185\n",
      "Iteration 11 | Episodes: 11 | Mean Return: -84.3002 | Std Return: 57.3277 | Mean steps: 191.0909 | Std steps: 31.0204 | Mean losses: total: 74.365275, policy: 0.001045, value: 74.373872, entropy: 0.009642\n",
      "Original advantages - mean: -4.6207, std: 12.6874\n",
      "Normalized advantages - mean: 0.0000, std: 12.6874\n",
      "Iteration 12 | Episodes: 12 | Mean Return: -68.2362 | Std Return: 73.7359 | Mean steps: 173.4167 | Std steps: 62.1080 | Mean losses: total: 90.131549, policy: -0.009904, value: 90.150921, entropy: 0.009468\n",
      "Original advantages - mean: -4.1730, std: 12.8279\n",
      "Normalized advantages - mean: 0.0000, std: 12.8279\n",
      "Iteration 13 | Episodes: 12 | Mean Return: -72.6923 | Std Return: 66.8278 | Mean steps: 184.5000 | Std steps: 37.2861 | Mean losses: total: 90.916931, policy: -0.032949, value: 90.959392, entropy: 0.009512\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\_Thesis\\warehouse-bot-training\\PPO_algorithm.py:183\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(self, env, iterations)\u001b[0m\n\u001b[0;32m    181\u001b[0m     obs_tensor \u001b[38;5;241m=\u001b[39m {key: th\u001b[38;5;241m.\u001b[39mtensor(obs[key], dtype\u001b[38;5;241m=\u001b[39mth\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m obs\u001b[38;5;241m.\u001b[39mkeys()}\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 183\u001b[0m     obs_tensor \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mtensor(obs, dtype\u001b[38;5;241m=\u001b[39mth\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    185\u001b[0m action, logp, _, value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mget_action(obs_tensor)\n\u001b[0;32m    187\u001b[0m next_obs, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action\u001b[38;5;241m.\u001b[39mitem())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent.train(env, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train(env, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train(env, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train(env, 25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
