{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch as th\n",
    "from torch import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_fork = multiprocessing.get_start_method() == \"fork\"\n",
    "\n",
    "device = (\n",
    "    th.device(0)\n",
    "    if th.cuda.is_available() and not is_fork\n",
    "    else th.device(\"cpu\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform environment from `mlagents` to `gymnasium`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.1\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "print(gym.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlagents_envs.side_channel.engine_configuration_channel import EngineConfigurationChannel\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from gymnasium.wrappers import NormalizeObservation, NormalizeReward\n",
    "\n",
    "from src.environments.env_camera_raycasts_gymnasium_wrapper import UnityCameraRaycastsGymWrapper\n",
    "\n",
    "env_path = \"D:/_Thesis/warehouse-bot-training/environment_builds/stage2/find_camera_raycasts_16x5/Warehouse_Bot.exe\"\n",
    "def make_env():\n",
    "\n",
    "  channel = EngineConfigurationChannel()\n",
    "\n",
    "  unity_env = UnityEnvironment(\n",
    "    file_name=env_path,\n",
    "    side_channels=[channel],\n",
    "    no_graphics=True\n",
    "  )\n",
    "  \n",
    "  channel.set_configuration_parameters(time_scale=1)\n",
    "  \n",
    "  gymnasium_env = UnityCameraRaycastsGymWrapper(unity_env)\n",
    "  \n",
    "  # Add reward normalization\n",
    "  # gymnasium_env = NormalizeReward(gymnasium_env)\n",
    "  \n",
    "  print(gymnasium_env.observation_space)\n",
    "  \n",
    "  return gymnasium_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# Actor-Critic Network for only vector observations\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super().__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.policy_head = nn.Linear(64, act_dim)\n",
    "        self.value_head = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.shared(x)\n",
    "        return self.policy_head(x), self.value_head(x)\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        logits, value = self.forward(obs)\n",
    "        dist = Categorical(logits=logits)\n",
    "        action = dist.sample()\n",
    "        return action, dist.log_prob(action), dist.entropy(), value.squeeze()\n",
    "\n",
    "    def evaluate_actions(self, obs, actions):\n",
    "        logits, values = self.forward(obs)\n",
    "        dist = Categorical(logits=logits)\n",
    "        log_probs = dist.log_prob(actions)\n",
    "        entropy = dist.entropy()\n",
    "        return log_probs, entropy, values.squeeze()\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * th.sigmoid(x)\n",
    "\n",
    "# Actor-Critic Network for multimodal observations (image plus vector)\n",
    "class ActorCriticMultimodal(nn.Module):\n",
    "    def __init__(self, act_dim, visual_size=[3, 36, 64], vector_obs_size=128):\n",
    "        super().__init__()\n",
    "        bands = visual_size[0]\n",
    "\n",
    "        # Shapes of image and vector inputs: [<batch size>, <bands, height, width>], [<batch size>, <length>]\n",
    "\n",
    "        visual_out_size = 64\n",
    "        vector_out_size = 32\n",
    "\n",
    "        # Visual Encoder\n",
    "        self.visual_encoder_cnn = nn.Sequential(\n",
    "            nn.Conv2d(bands, 16, kernel_size=5, stride=4, padding=0),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        # Compute flattened visual output size from dummy input\n",
    "        dummy_input = th.zeros(1, bands, visual_size[1], visual_size[2])\n",
    "        with th.no_grad():\n",
    "            visual_encoder_cnn_out_size = self.visual_encoder_cnn(dummy_input).shape[1]\n",
    "\n",
    "        self.visual_encoder_mlp = nn.Sequential(\n",
    "            nn.Linear(visual_encoder_cnn_out_size, 64),\n",
    "            Swish(),\n",
    "            nn.Linear(64, visual_out_size),\n",
    "            Swish()\n",
    "        )\n",
    "        \n",
    "        # Vector Encoder\n",
    "        self.vector_encoder = nn.Sequential(\n",
    "            nn.Linear(vector_obs_size, 32),\n",
    "            Swish(),\n",
    "            nn.Linear(32, vector_out_size),                             \n",
    "            Swish()\n",
    "        )\n",
    "\n",
    "        # Concatenation Network\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(visual_out_size + vector_out_size, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.policy_head = nn.Linear(32, act_dim)\n",
    "        self.value_head = nn.Linear(32, 1)\n",
    "\n",
    "    \n",
    "    def forward(self, observations):\n",
    "        image = observations[\"image\"].float()\n",
    "        vector = observations[\"vector\"]\n",
    "\n",
    "        image_features = self.visual_encoder_cnn(image)\n",
    "        image_features = self.visual_encoder_mlp(image_features)\n",
    "        vector_features = self.vector_encoder(vector)\n",
    "\n",
    "        combined = th.cat([image_features, vector_features], dim=1)\n",
    "        x = self.shared(combined)\n",
    "        return self.policy_head(x), self.value_head(x)\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        logits, value = self.forward(obs)\n",
    "        dist = Categorical(logits=logits)\n",
    "        action = dist.sample()\n",
    "        return action, dist.log_prob(action), dist.entropy(), value.squeeze()\n",
    "\n",
    "    def evaluate_actions(self, obs, actions):\n",
    "        logits, values = self.forward(obs)\n",
    "        dist = Categorical(logits=logits)\n",
    "        log_probs = dist.log_prob(actions)\n",
    "        entropy = dist.entropy()\n",
    "        return log_probs, entropy, values.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"\n",
    "    Count parameters in each block of the network and total parameters.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing parameter counts for each block and total\n",
    "    \"\"\"\n",
    "    total_params = 0\n",
    "    block_params = {}\n",
    "    \n",
    "    for name, module in model.named_children():\n",
    "        params = sum(p.numel() for p in module.parameters())\n",
    "        block_params[name] = params\n",
    "        total_params += params\n",
    "        \n",
    "    block_params['total'] = total_params\n",
    "    return block_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create settings dictionary\n",
    "settings = {\n",
    "    'gamma': 0.99,\n",
    "    'lam': 0.95,\n",
    "    'clip_eps': 0.2,\n",
    "    'ppo_epochs': 2,\n",
    "    'batch_size': 64,\n",
    "    'update_timesteps': 512,\n",
    "    'lr': 1e-4,\n",
    "    'val_loss_coef': 0.5 / 100,\n",
    "    'ent_loss_coef':  0.001,\n",
    "    'device': th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict('image': Box(0, 255, (3, 36, 64), uint8), 'vector': Box(0.0, 255.0, (80,), float32))\n"
     ]
    }
   ],
   "source": [
    "# Create environment\n",
    "env = make_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'visual_encoder_cnn': 15104, 'visual_encoder_mlp': 20608, 'vector_encoder': 3648, 'shared': 8288, 'policy_head': 99, 'value_head': 33, 'total': 47780}\n"
     ]
    }
   ],
   "source": [
    "act_dim = env.action_space.n\n",
    "\n",
    "model_net = ActorCriticMultimodal(act_dim, visual_size=[3, 36, 64], vector_obs_size=80)\n",
    "param_counts = count_parameters(model_net)\n",
    "print(param_counts)\n",
    "\n",
    "from src.algorithms.PPO_algorithm import PPOAgent\n",
    "agent = PPOAgent(model_net, settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 completed. Episodes: 8 | Mean Return: -83.5684 | Std Return: 6.3631 | Mean steps: 70.7500 | Std steps: 9.8203 | Mean losses: total: 0.004701, policy: 0.000814, value: 0.004985, entropy: 0.001098\n",
      "Iteration 1 completed. Episodes: 8 | Mean Return: -19.4160 | Std Return: 92.6767 | Mean steps: 67.8750 | Std steps: 35.2187 | Mean losses: total: 0.007368, policy: 0.003493, value: 0.004974, entropy: 0.001098\n",
      "Iteration 2 completed. Episodes: 7 | Mean Return: -45.9314 | Std Return: 61.1068 | Mean steps: 84.5714 | Std steps: 37.3396 | Mean losses: total: -0.008490, policy: -0.013262, value: 0.005870, entropy: 0.001099\n",
      "Iteration 3 completed. Episodes: 7 | Mean Return: -61.0048 | Std Return: 66.1800 | Mean steps: 84.8571 | Std steps: 34.7868 | Mean losses: total: -0.005278, policy: -0.009408, value: 0.005227, entropy: 0.001098\n",
      "Iteration 4 completed. Episodes: 8 | Mean Return: -35.1805 | Std Return: 80.5388 | Mean steps: 79.2500 | Std steps: 28.6564 | Mean losses: total: 0.003565, policy: -0.000338, value: 0.004999, entropy: 0.001097\n",
      "Iteration 5 completed. Episodes: 7 | Mean Return: -58.7511 | Std Return: 62.6943 | Mean steps: 78.4286 | Std steps: 14.3811 | Mean losses: total: 0.009561, policy: 0.005729, value: 0.004928, entropy: 0.001096\n",
      "Iteration 6 completed. Episodes: 6 | Mean Return: -84.4640 | Std Return: 11.6862 | Mean steps: 91.3333 | Std steps: 18.9444 | Mean losses: total: 0.003853, policy: -0.000057, value: 0.005001, entropy: 0.001091\n",
      "Iteration 7 completed. Episodes: 6 | Mean Return: -55.9260 | Std Return: 70.9905 | Mean steps: 85.5000 | Std steps: 44.2822 | Mean losses: total: 0.033264, policy: 0.029859, value: 0.004491, entropy: 0.001086\n",
      "Iteration 8 completed. Episodes: 5 | Mean Return: -89.7699 | Std Return: 5.6247 | Mean steps: 107.4000 | Std steps: 11.8423 | Mean losses: total: -0.000744, policy: -0.004551, value: 0.004895, entropy: 0.001088\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\_Thesis\\warehouse-bot-training\\src\\algorithms\\PPO_algorithm.py:181\u001b[0m, in \u001b[0;36mPPOAgent.train\u001b[1;34m(self, env, iterations)\u001b[0m\n\u001b[0;32m    177\u001b[0m     obs_tensor \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mtensor(obs, dtype\u001b[38;5;241m=\u001b[39mth\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    179\u001b[0m action, logp, _, value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mget_action(obs_tensor)\n\u001b[1;32m--> 181\u001b[0m next_obs, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[0;32m    184\u001b[0m \u001b[38;5;66;03m# Remove batch dimension from observation tensors for buffer storage\u001b[39;00m\n",
      "File \u001b[1;32md:\\_Thesis\\warehouse-bot-training\\src\\environments\\env_camera_raycasts_gymnasium_wrapper.py:44\u001b[0m, in \u001b[0;36mUnityCameraRaycastsGymWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     41\u001b[0m     action_tuple\u001b[38;5;241m.\u001b[39madd_discrete(np\u001b[38;5;241m.\u001b[39marray(action)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munity_env\u001b[38;5;241m.\u001b[39mset_action_for_agent(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbehavior_name, \u001b[38;5;241m0\u001b[39m, action_tuple)\n\u001b[1;32m---> 44\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munity_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m decision_steps, terminal_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munity_env\u001b[38;5;241m.\u001b[39mget_steps(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbehavior_name)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01min\u001b[39;00m terminal_steps:\n",
      "File \u001b[1;32mc:\\Users\\Pawel\\anaconda3\\envs\\rl_env\\lib\\site-packages\\mlagents_envs\\timers.py:305\u001b[0m, in \u001b[0;36mtimed.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hierarchical_timer(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m):\n\u001b[1;32m--> 305\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Pawel\\anaconda3\\envs\\rl_env\\lib\\site-packages\\mlagents_envs\\environment.py:348\u001b[0m, in \u001b[0;36mUnityEnvironment.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    346\u001b[0m step_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_step_input(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env_actions)\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m hierarchical_timer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcommunicator.exchange\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 348\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_communicator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexchange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll_process\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    350\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnityCommunicatorStoppedException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCommunicator has exited.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Pawel\\anaconda3\\envs\\rl_env\\lib\\site-packages\\mlagents_envs\\rpc_communicator.py:142\u001b[0m, in \u001b[0;36mRpcCommunicator.exchange\u001b[1;34m(self, inputs, poll_callback)\u001b[0m\n\u001b[0;32m    140\u001b[0m message\u001b[38;5;241m.\u001b[39munity_input\u001b[38;5;241m.\u001b[39mCopyFrom(inputs)\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munity_to_external\u001b[38;5;241m.\u001b[39mparent_conn\u001b[38;5;241m.\u001b[39msend(message)\n\u001b[1;32m--> 142\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll_for_timeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoll_callback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munity_to_external\u001b[38;5;241m.\u001b[39mparent_conn\u001b[38;5;241m.\u001b[39mrecv()\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mheader\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Pawel\\anaconda3\\envs\\rl_env\\lib\\site-packages\\mlagents_envs\\rpc_communicator.py:106\u001b[0m, in \u001b[0;36mRpcCommunicator.poll_for_timeout\u001b[1;34m(self, poll_callback)\u001b[0m\n\u001b[0;32m    104\u001b[0m callback_timeout_wait \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout_wait \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m<\u001b[39m deadline:\n\u001b[1;32m--> 106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munity_to_external\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent_conn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallback_timeout_wait\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    107\u001b[0m         \u001b[38;5;66;03m# Got an acknowledgment from the connection\u001b[39;00m\n\u001b[0;32m    108\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m poll_callback:\n\u001b[0;32m    110\u001b[0m         \u001b[38;5;66;03m# Fire the callback - if it detects something wrong, it should raise an exception.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Pawel\\anaconda3\\envs\\rl_env\\lib\\multiprocessing\\connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[1;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Pawel\\anaconda3\\envs\\rl_env\\lib\\multiprocessing\\connection.py:330\u001b[0m, in \u001b[0;36mPipeConnection._poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_got_empty_message \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m    328\u001b[0m             _winapi\u001b[38;5;241m.\u001b[39mPeekNamedPipe(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 330\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\Pawel\\anaconda3\\envs\\rl_env\\lib\\multiprocessing\\connection.py:879\u001b[0m, in \u001b[0;36mwait\u001b[1;34m(object_list, timeout)\u001b[0m\n\u001b[0;32m    876\u001b[0m                 ready_objects\u001b[38;5;241m.\u001b[39madd(o)\n\u001b[0;32m    877\u001b[0m                 timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 879\u001b[0m     ready_handles \u001b[38;5;241m=\u001b[39m \u001b[43m_exhaustive_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaithandle_to_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m     \u001b[38;5;66;03m# request that overlapped reads stop\u001b[39;00m\n\u001b[0;32m    882\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ov \u001b[38;5;129;01min\u001b[39;00m ov_list:\n",
      "File \u001b[1;32mc:\\Users\\Pawel\\anaconda3\\envs\\rl_env\\lib\\multiprocessing\\connection.py:811\u001b[0m, in \u001b[0;36m_exhaustive_wait\u001b[1;34m(handles, timeout)\u001b[0m\n\u001b[0;32m    809\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    810\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m L:\n\u001b[1;32m--> 811\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWaitForMultipleObjects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    812\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;241m==\u001b[39m WAIT_TIMEOUT:\n\u001b[0;32m    813\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent.train(env, 25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
